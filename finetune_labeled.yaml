apiVersion: batch/v1
kind: Job
metadata:
  name: chord-finetune-labeled
  namespace: csuf-titans
  labels:
    app: chord-finetune
spec:
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: chord-finetune
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - NVIDIA-GeForce-RTX-3090
      restartPolicy: Never
      tolerations:
        - key: "node.kubernetes.io/not-ready"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
        - key: "node.kubernetes.io/unreachable"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
      initContainers:
        - name: clone-repo
          image: alpine:3.16
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              set -ex
              echo "Installing git..."
              apk update && apk add --no-cache git
              echo "Checking for existing project repository..."
              if [ ! -d "/mnt/storage/ChordMini" ]; then
                echo "Cloning repository..."
                git clone https://gitlab.com/ptnghia-j/ChordMini.git /mnt/storage/ChordMini || {
                  echo "ERROR: Failed to clone repository"
                  exit 1
                }
              else
                echo "Repository already exists."
              fi
              exit 0
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
        - name: data-check
          image: alpine:3.16
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              set -ex
              echo "Checking data directory structure..."
              
              # Locate the project root
              PROJ_ROOT="/mnt/storage/ChordMini"
              echo "Project root: $PROJ_ROOT"
              
              # Check and create main data directories
              echo "Checking/creating data directories..."
              for dir in "/mnt/storage/data/LabeledDataset/Audio" "/mnt/storage/data/LabeledDataset/Labels" "/mnt/storage/data/LabeledDataset/cache"; do
                if [ -d "$dir" ]; then
                  echo "Directory already exists: $dir"
                else
                  mkdir -p "$dir"
                  echo "Created directory: $dir"
                fi
              done
              
              # Check and create audio dataset subdirectories
              echo "Checking/creating audio dataset subdirectories..."
              for subdir in "billboard" "caroleKing" "queen" "theBeatles"; do
                dir="/mnt/storage/data/LabeledDataset/Audio/$subdir"
                if [ -d "$dir" ]; then
                  echo "Audio directory already exists: $subdir"
                else
                  mkdir -p "$dir"
                  echo "Created audio directory: $subdir"
                fi
              done
              
              # Check and create label dataset subdirectories
              echo "Checking/creating label dataset subdirectories..."
              for subdir in "billboardLabels" "caroleKingLabels" "queenLabels" "theBeatlesLabels"; do
                dir="/mnt/storage/data/LabeledDataset/Labels/$subdir"
                if [ -d "$dir" ]; then
                  echo "Label directory already exists: $subdir"
                else
                  mkdir -p "$dir"
                  echo "Created label directory: $subdir"
                fi
              done
              
              # Create symlinks from project directory to actual data location
              mkdir -p "$PROJ_ROOT/data/LabeledDataset"
              
              # Create unified symlinks for LabeledDataset
              if [ -L "$PROJ_ROOT/data/LabeledDataset/Audio" ]; then
                echo "Symlink for Audio already exists"
              else
                ln -sf "/mnt/storage/data/LabeledDataset/Audio" "$PROJ_ROOT/data/LabeledDataset/Audio"
                echo "Created symlink for Audio"
              fi
              
              if [ -L "$PROJ_ROOT/data/LabeledDataset/Labels" ]; then
                echo "Symlink for Labels already exists"
              else
                ln -sf "/mnt/storage/data/LabeledDataset/Labels" "$PROJ_ROOT/data/LabeledDataset/Labels"
                echo "Created symlink for Labels"
              fi
              
              if [ -L "$PROJ_ROOT/data/LabeledDataset/cache" ]; then
                echo "Symlink for cache already exists"
              else
                ln -sf "/mnt/storage/data/LabeledDataset/cache" "$PROJ_ROOT/data/LabeledDataset/cache"
                echo "Created symlink for cache"
              fi
              
              # Check for audio files
              echo "---- Checking for audio files ----"
              find "/mnt/storage/data/LabeledDataset/Audio" -name "*.mp3" -o -name "*.wav" | wc -l
              
              # Check for label files
              echo "---- Checking for label files ----"
              find "/mnt/storage/data/LabeledDataset/Labels" -name "*.lab" -o -name "*.txt" | wc -l
              
              echo "Data directory structure prepared."
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      containers:
        - name: finetune-trainer
          image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime
          workingDir: /mnt/storage/ChordMini
          imagePullPolicy: IfNotPresent
          env:
            # These environment variables override defaults in student_config.yaml
            - name: PYTHONPATH
              value: /mnt/storage/ChordMini
            # Model scale - overrides student_config.yaml
            - name: MODEL_SCALE
              value: "1.0"  # Default scale: 1.0 = base model
            # Learning rate parameters
            - name: LEARNING_RATE
              value: "0.0002"  # Lower base learning rate for fine-tuning
            - name: MIN_LEARNING_RATE
              value: "0.00001"  # Minimum learning rate
            # Knowledge distillation settings
            - name: USE_KD_LOSS
              value: "false"  # Set to "true" to enable KD loss
            - name: KD_ALPHA
              value: "0.3"  # Weight for KD loss (0-1)
            - name: TEMPERATURE
              value: "2.0"  # Temperature for softening distributions
            # Focal loss settings
            - name: USE_FOCAL_LOSS
              value: "true"  # Set to "true" to enable focal loss
            - name: FOCAL_GAMMA
              value: "2.0"   # Focusing parameter
            - name: FOCAL_ALPHA
              value: "0.3"   # Class weight parameter
            # Warmup settings
            - name: USE_WARMUP
              value: "true"  # Set to "true" to enable learning rate warm-up
            - name: WARMUP_START_LR
              value: "0.00005"  # Starting learning rate for warm-up
            - name: WARMUP_END_LR
              value: "0.0002"  # Final learning rate after warm-up
            - name: WARMUP_EPOCHS
              value: "5"  # Number of epochs for warm-up
            # Dropout setting
            - name: DROPOUT
              value: "0.3"  # Set dropout probability (0-1)
            # Data paths
            - name: DATA_ROOT
              value: "/mnt/storage/data"
            # Fine-tuning specific settings
            - name: PRETRAINED_MODEL
              value: "/mnt/storage/checkpoints/student/student_model_best.pth"  # Path to pretrained model
            - name: FREEZE_FEATURE_EXTRACTOR
              value: "false"  # Whether to freeze feature extraction layers
            - name: EPOCHS
              value: "50"  # Number of fine-tuning epochs
            - name: BATCH_SIZE
              value: "32"  # Batch size for training
            # LR schedule
            - name: LR_SCHEDULE
              value: "cosine"  # LR schedule type
            # GPU settings
            - name: GPU_MEMORY_FRACTION
              value: "0.95"  # Use 95% of available GPU memory
            - name: CUDA_VISIBLE_DEVICES
              value: "0"
            # Dataset caching settings
            - name: DISABLE_CACHE
              value: "false"  # Use caching for features
            - name: METADATA_CACHE
              value: "false"  # Don't use metadata-only caching
            # Checkpoint settings
            - name: SAVE_DIR
              value: "/mnt/storage/checkpoints/finetune"  # Directory to save checkpoints
            # Cross-validation settings
            - name: USE_CROSS_VALIDATION
              value: "false"  # Set to "true" to enable cross-validation
            - name: KFOLD
              value: "0"      # Which fold to use for validation (0-4)
            - name: TOTAL_FOLDS
              value: "5"      # Total number of folds
            - name: PREPROCESS
              value: "false"  # Set to "true" to preprocess and generate features
          command:
            - sh
            - -c
            - |
              set -ex
              echo "=== Fine-tuning model on labeled data ==="
              
              # Install necessary system packages
              apt-get update && apt-get install -y libsndfile1 || { echo "ERROR: Failed to install system dependencies"; exit 1; }
              
              # Create a minimal requirements.txt if missing
              if [ ! -f requirements.txt ]; then
                echo "Creating minimal requirements.txt"
                echo "numpy==1.22.0" > requirements.txt
                echo "librosa>=0.8.0" >> requirements.txt
                echo "torch>=1.9.0" >> requirements.txt
                echo "tqdm>=4.62.0" >> requirements.txt
                echo "scikit-learn>=1.0.0" >> requirements.txt
                echo "pyyaml>=6.0.0" >> requirements.txt  # Added for config loading
                echo "soundfile>=0.10.0" >> requirements.txt  # For audio file loading
              fi
              
              pip install --no-cache-dir -r requirements.txt || { echo "WARNING: Some requirements may have failed"; }
              pip install --no-cache-dir matplotlib tqdm pyyaml librosa scikit-learn soundfile || { echo "ERROR: Failed to install additional Python packages"; exit 1; }
              
              # Verify the data paths
              echo "Checking LabeledDataset paths..."
              
              # Create checkpoints directory
              mkdir -p ${SAVE_DIR}
              
              # Check for audio/label files
              echo "Checking for audio files..."
              find ${DATA_ROOT}/LabeledDataset/Audio -type f | grep -E '\.(mp3|wav)$' | wc -l
              
              echo "Checking for label files..."
              find ${DATA_ROOT}/LabeledDataset/Labels -type f | grep -E '\.(lab|txt)$' | wc -l
              
              # Prepare command line arguments
              KD_ARGS=""
              if [ "${USE_KD_LOSS}" = "true" ]; then
                KD_ARGS="--use_kd_loss --kd_alpha ${KD_ALPHA} --temperature ${TEMPERATURE}"
              fi
              
              FOCAL_ARGS=""
              if [ "${USE_FOCAL_LOSS}" = "true" ]; then
                FOCAL_ARGS="--use_focal_loss --focal_gamma ${FOCAL_GAMMA}"
                if [ "${FOCAL_ALPHA}" != "" ]; then
                  FOCAL_ARGS="${FOCAL_ARGS} --focal_alpha ${FOCAL_ALPHA}"
                fi
              fi
              
              WARMUP_ARGS=""
              if [ "${USE_WARMUP}" = "true" ]; then
                WARMUP_ARGS="--use_warmup --warmup_epochs ${WARMUP_EPOCHS} --warmup_start_lr ${WARMUP_START_LR} --warmup_end_lr ${WARMUP_END_LR}"
              fi
              
              FREEZE_ARGS=""
              if [ "${FREEZE_FEATURE_EXTRACTOR}" = "true" ]; then
                FREEZE_ARGS="--freeze_feature_extractor"
              fi
              
              CACHE_ARGS=""
              if [ "${DISABLE_CACHE}" = "true" ]; then
                CACHE_ARGS="--disable_cache"
              fi
              
              if [ "${METADATA_CACHE}" = "true" ]; then
                CACHE_ARGS="${CACHE_ARGS} --metadata_cache"
              fi
              
              # Define directories
              AUDIO_DIRS="${DATA_ROOT}/LabeledDataset/Audio/billboard ${DATA_ROOT}/LabeledDataset/Audio/caroleKing ${DATA_ROOT}/LabeledDataset/Audio/queen ${DATA_ROOT}/LabeledDataset/Audio/theBeatles"
              LABEL_DIRS="${DATA_ROOT}/LabeledDataset/Labels/billboardLabels ${DATA_ROOT}/LabeledDataset/Labels/caroleKingLabels ${DATA_ROOT}/LabeledDataset/Labels/queenLabels ${DATA_ROOT}/LabeledDataset/Labels/theBeatlesLabels"
              CACHE_DIR="${DATA_ROOT}/LabeledDataset/cache"
              
              # Cross-validation arguments
              CV_ARGS=""
              if [ "${USE_CROSS_VALIDATION}" = "true" ]; then
                echo "Using cross-validation with fold ${KFOLD} of ${TOTAL_FOLDS}"
                if [ "${PREPROCESS}" = "true" ]; then
                  # Run preprocessing first if requested
                  echo "Running preprocessing step to generate features..."
                  python train_cv_kd.py --config ./config/student_config.yaml \
                    --kfold ${KFOLD} \
                    --total_folds ${TOTAL_FOLDS} \
                    --audio_dirs ${AUDIO_DIRS} \
                    --label_dirs ${LABEL_DIRS} \
                    --cache_dir ${CACHE_DIR} \
                    --preprocess
                  
                  echo "Preprocessing completed."
                fi
                
                # Use cross-validation training script instead of standard fine-tuning
                echo "Starting cross-validation training with fold ${KFOLD}..."
                python train_cv_kd.py --config ./config/student_config.yaml \
                  --kfold ${KFOLD} \
                  --total_folds ${TOTAL_FOLDS} \
                  --save_dir ${SAVE_DIR}/cv_fold${KFOLD} \
                  --audio_dirs ${AUDIO_DIRS} \
                  --label_dirs ${LABEL_DIRS} \
                  --cache_dir ${CACHE_DIR} \
                  --learning_rate ${LEARNING_RATE} \
                  --batch_size ${BATCH_SIZE} \
                  ${KD_ARGS}
              else
                # Run the standard fine-tuning script with all options
                echo "Starting fine-tuning with all options..."
                python train_finetune.py --config ./config/student_config.yaml \
                  --pretrained ${PRETRAINED_MODEL} \
                  --save_dir ${SAVE_DIR} \
                  --audio_dirs ${AUDIO_DIRS} \
                  --label_dirs ${LABEL_DIRS} \
                  --cache_dir ${CACHE_DIR} \
                  --learning_rate ${LEARNING_RATE} \
                  --min_learning_rate ${MIN_LEARNING_RATE} \
                  --model_scale ${MODEL_SCALE} \
                  --dropout ${DROPOUT} \
                  --epochs ${EPOCHS} \
                  --batch_size ${BATCH_SIZE} \
                  --lr_schedule ${LR_SCHEDULE} \
                  --gpu_memory_fraction ${GPU_MEMORY_FRACTION} \
                  ${WARMUP_ARGS} ${KD_ARGS} ${FOCAL_ARGS} ${FREEZE_ARGS} ${CACHE_ARGS}
              fi
              
              echo "=== Training complete ==="
              
              # Clear GPU cache to release memory before exiting
              python -c "import torch; torch.cuda.empty_cache() if torch.cuda.is_available() else None"
          resources:
            requests:
              cpu: "5"             
              memory: "58Gi"    
              nvidia.com/gpu: "1"
            limits:
              cpu: "10"             
              memory: "108Gi" 
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      volumes:
        - name: storage
          persistentVolumeClaim:
            claimName: temporary-storage
