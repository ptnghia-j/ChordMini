apiVersion: batch/v1
kind: Job
metadata:
  name: chord-finetune-labeled
  namespace: csuf-titans
  labels:
    app: chord-finetune
spec:
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: chord-finetune
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - NVIDIA-GeForce-RTX-3090
      restartPolicy: Never
      tolerations:
        - key: "node.kubernetes.io/not-ready"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
        - key: "node.kubernetes.io/unreachable"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
      initContainers:
        - name: clone-repo
          image: alpine:3.16
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              set -ex
              echo "Installing git..."
              apk update && apk add --no-cache git
              echo "Checking for existing project repository..."
              if [ ! -d "/mnt/storage/ChordMini" ]; then
                echo "Cloning repository..."
                git clone https://gitlab.com/ptnghia-j/ChordMini.git /mnt/storage/ChordMini || {
                  echo "ERROR: Failed to clone repository"
                  exit 1
                }
              else
                echo "Repository already exists."
              fi
              exit 0
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
        - name: data-check
          image: alpine:3.16
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              set -ex
              echo "Checking data directory structure..."
              
              # Locate the project root
              PROJ_ROOT="/mnt/storage/ChordMini"
              echo "Project root: $PROJ_ROOT"
              
              # Check and create main data directories
              echo "Checking/creating data directories..."
              for dir in "/mnt/storage/data/LabeledDataset/Audio" "/mnt/storage/data/LabeledDataset/Labels" "/mnt/storage/data/LabeledDataset/cache"; do
                if [ -d "$dir" ]; then
                  echo "Directory already exists: $dir"
                else
                  mkdir -p "$dir"
                  echo "Created directory: $dir"
                fi
              done
              
              # Check and create audio dataset subdirectories
              echo "Checking/creating audio dataset subdirectories..."
              for subdir in "billboard" "caroleKing" "queen" "theBeatles"; do
                dir="/mnt/storage/data/LabeledDataset/Audio/$subdir"
                if [ -d "$dir" ]; then
                  echo "Audio directory already exists: $subdir"
                else
                  mkdir -p "$dir"
                  echo "Created audio directory: $subdir"
                fi
              done
              
              # Check and create label dataset subdirectories
              echo "Checking/creating label dataset subdirectories..."
              for subdir in "billboardLabels" "caroleKingLabels" "queenLabels" "theBeatlesLabels"; do
                dir="/mnt/storage/data/LabeledDataset/Labels/$subdir"
                if [ -d "$dir" ]; then
                  echo "Label directory already exists: $subdir"
                else
                  mkdir -p "$dir"
                  echo "Created label directory: $subdir"
                fi
              done
              
              # Create symlinks from project directory to actual data location
              mkdir -p "$PROJ_ROOT/data/LabeledDataset"
              
              # Create unified symlinks for LabeledDataset
              if [ -L "$PROJ_ROOT/data/LabeledDataset/Audio" ]; then
                echo "Symlink for Audio already exists"
              else
                ln -sf "/mnt/storage/data/LabeledDataset/Audio" "$PROJ_ROOT/data/LabeledDataset/Audio"
                echo "Created symlink for Audio"
              fi
              
              if [ -L "$PROJ_ROOT/data/LabeledDataset/Labels" ]; then
                echo "Symlink for Labels already exists"
              else
                ln -sf "/mnt/storage/data/LabeledDataset/Labels" "$PROJ_ROOT/data/LabeledDataset/Labels"
                echo "Created symlink for Labels"
              fi
              
              if [ -L "$PROJ_ROOT/data/LabeledDataset/cache" ]; then
                echo "Symlink for cache already exists"
              else
                ln -sf "/mnt/storage/data/LabeledDataset/cache" "$PROJ_ROOT/data/LabeledDataset/cache"
                echo "Created symlink for cache"
              fi
              
              # Check for audio files
              echo "---- Checking for audio files ----"
              find "/mnt/storage/data/LabeledDataset/Audio" -name "*.mp3" -o -name "*.wav" | wc -l
              
              # Check for label files
              echo "---- Checking for label files ----"
              find "/mnt/storage/data/LabeledDataset/Labels" -name "*.lab" -o -name "*.txt" | wc -l
              
              echo "Data directory structure prepared."
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      containers:
        - name: finetune-trainer
          image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime
          workingDir: /mnt/storage/ChordMini
          imagePullPolicy: IfNotPresent
          env:
            # These environment variables override defaults in student_config.yaml
            - name: PYTHONPATH
              value: /mnt/storage/ChordMini
            # Model scale - overrides student_config.yaml
            - name: MODEL_SCALE
              value: "1.0"  # Default scale: 1.0 = base model
            # Learning rate parameters
            - name: LEARNING_RATE
              value: "0.0001"  # Lower base learning rate for fine-tuning
            - name: MIN_LEARNING_RATE
              value: "0.00001"  # Minimum learning rate
            # Knowledge distillation settings
            - name: USE_KD_LOSS
              value: "false"  # Set to "true" to enable KD loss
            # Add explicit note about KD status
            - name: KD_STATUS_CHECK
              value: "disabled_explicitly"  # For debugging purposes
            - name: KD_ALPHA
              value: "0.3"  # Weight for KD loss (0-1) - only used if KD is enabled
            - name: TEMPERATURE
              value: "2.0"  # Temperature for softening distributions
            # Focal loss settings
            - name: USE_FOCAL_LOSS
              value: "true"  # Set to "true" to enable focal loss
            - name: FOCAL_GAMMA
              value: "2.0"   # Focusing parameter
            - name: FOCAL_ALPHA
              value: "0.25"   # Class weight parameter
            # Warmup settings
            - name: USE_WARMUP
              value: "false"  # Set to "true" to enable learning rate warm-up
            - name: WARMUP_START_LR
              value: "0.00001"  # Starting learning rate for warm-up
            - name: WARMUP_END_LR
              value: "0.00004"  # Final learning rate after warm-up
            - name: WARMUP_EPOCHS
              value: "4"  # Number of epochs for warm-up
            # Dropout setting
            - name: DROPOUT
              value: "0.4"  # Set dropout probability (0-1)
            # Data paths
            - name: DATA_ROOT
              value: "/mnt/storage/data"
            # Model vocabulary size - crucial for matching pretrained model
            - name: NUM_CHORDS
              value: "170"  # Must match pretrained model's 170 chord classes
            - name: USE_VOCA
              value: "true"  # Use large vocabulary (170 chords)
            # Fine-tuning specific settings
            - name: PRETRAINED_MODEL
              value: "/mnt/storage/checkpoints/student/student_model_best.pth"  # Path to pretrained model
            - name: FREEZE_FEATURE_EXTRACTOR
              value: "false"  # Whether to freeze feature extraction layers
            - name: EPOCHS
              value: "100"  # Number of fine-tuning epochs
            - name: BATCH_SIZE
              value: "32"  # Batch size for training
            # LR schedule
            - name: LR_SCHEDULE
              value: "validation"  # LR schedule type
            # GPU settings
            - name: GPU_MEMORY_FRACTION
              value: "0.95"  # Use 95% of available GPU memory
            - name: CUDA_VISIBLE_DEVICES
              value: "0"
            # Dataset caching settings
            - name: DISABLE_CACHE
              value: "false"  # Use caching for features
            - name: METADATA_CACHE
              value: "false"  # Don't use metadata-only caching
            # Checkpoint settings
            - name: SAVE_DIR
              value: "/mnt/storage/checkpoints/finetune"  # Directory to save checkpoints
            # Cross-validation settings
            - name: USE_CROSS_VALIDATION
              value: "false"  # Set to "true" to enable cross-validation
            - name: KFOLD
              value: "0"      # Which fold to use for validation (0-4)
            - name: TOTAL_FOLDS
              value: "5"      # Total number of folds
          command:
            - python
            - train_finetune.py
            - --config
            - ./config/student_config.yaml
            - --pretrained
            - /mnt/storage/checkpoints/student/student_model_best.pth
          resources:
            requests:
              cpu: "2"             
              memory: "16Gi"    
              nvidia.com/gpu: "1"
            limits:
              cpu: "5"             
              memory: "24Gi" 
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      volumes:
        - name: storage
          persistentVolumeClaim:
            claimName: temporary-storage
