apiVersion: batch/v1
kind: Job
metadata:
  name: chord-finetune # Updated name
  namespace: csuf-titans
  labels:
    app: chord-finetune-synthlike # Updated label
spec:
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: chord-finetune-synthlike # Updated label
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - NVIDIA-A10
      restartPolicy: Never
      tolerations:
        - key: "node.kubernetes.io/not-ready"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
        - key: "node.kubernetes.io/unreachable"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
      initContainers:
        - name: clone-repo
          image: alpine:3.16
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              set -ex
              echo "Installing git..."
              apk update && apk add --no-cache git
              echo "Checking for existing project repository..."
              if [ ! -d "/mnt/storage/ChordMini" ]; then
                echo "Cloning repository..."
                git clone https://gitlab.com/ptnghia-j/ChordMini.git /mnt/storage/ChordMini || {
                  echo "ERROR: Failed to clone repository"
                  exit 1
                }
              else
                echo "Repository already exists."
              fi
              exit 0
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
        - name: data-check
          image: alpine:3.16
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              set -ex
              echo "Checking data directory structure..."

              # Locate the project root
              PROJ_ROOT="/mnt/storage/ChordMini"
              echo "Project root: $PROJ_ROOT"

              # Check and create main data directories
              echo "Checking/creating data directories..."
              # Added LabeledDataset_synth cache
              for dir in "/mnt/storage/data/LabeledDataset/Audio" "/mnt/storage/data/LabeledDataset/Labels" "/mnt/storage/data/LabeledDataset/cache" \
                         "/mnt/storage/data/LabeledDataset_synth/spectrograms" "/mnt/storage/data/LabeledDataset_synth/logits" "/mnt/storage/data/LabeledDataset_synth/cache"; do
                if [ -d "$dir" ]; then
                  echo "Directory already exists: $dir"
                else
                  mkdir -p "$dir"
                  echo "Created directory: $dir"
                fi
              done

              # Check and create audio dataset subdirectories (still needed for reference)
              echo "Checking/creating audio dataset subdirectories..."
              for subdir in "billboard" "caroleKing" "queen" "theBeatles"; do
                dir="/mnt/storage/data/LabeledDataset/Audio/$subdir"
                if [ -d "$dir" ]; then
                  echo "Audio directory already exists: $subdir"
                else
                  mkdir -p "$dir"
                  echo "Created audio directory: $subdir"
                fi
              done

              # Check and create label dataset subdirectories (REAL labels)
              echo "Checking/creating label dataset subdirectories..."
              for subdir in "billboardLabels" "caroleKingLabels" "queenLabels" "theBeatlesLabels"; do
                dir="/mnt/storage/data/LabeledDataset/Labels/$subdir"
                if [ -d "$dir" ]; then
                  echo "Label directory already exists: $subdir"
                else
                  mkdir -p "$dir"
                  echo "Created label directory: $subdir"
                fi
              done

              # Create symlinks from project directory to actual data location
              mkdir -p "$PROJ_ROOT/data/LabeledDataset"
              mkdir -p "$PROJ_ROOT/data/LabeledDataset_synth"

              # Create unified symlinks for LabeledDataset (Audio/Labels/cache)
              if [ -L "$PROJ_ROOT/data/LabeledDataset/Audio" ]; then echo "Symlink for Audio already exists"; else ln -sf "/mnt/storage/data/LabeledDataset/Audio" "$PROJ_ROOT/data/LabeledDataset/Audio"; echo "Created symlink for Audio"; fi
              if [ -L "$PROJ_ROOT/data/LabeledDataset/Labels" ]; then echo "Symlink for Labels already exists"; else ln -sf "/mnt/storage/data/LabeledDataset/Labels" "$PROJ_ROOT/data/LabeledDataset/Labels"; echo "Created symlink for Labels"; fi
              if [ -L "$PROJ_ROOT/data/LabeledDataset/cache" ]; then echo "Symlink for LabeledDataset cache already exists"; else ln -sf "/mnt/storage/data/LabeledDataset/cache" "$PROJ_ROOT/data/LabeledDataset/cache"; echo "Created symlink for LabeledDataset cache"; fi

              # Create unified symlinks for LabeledDataset_synth (spectrograms/logits/cache)
              if [ -L "$PROJ_ROOT/data/LabeledDataset_synth/spectrograms" ]; then echo "Symlink for spectrograms already exists"; else ln -sf "/mnt/storage/data/LabeledDataset_synth/spectrograms" "$PROJ_ROOT/data/LabeledDataset_synth/spectrograms"; echo "Created symlink for spectrograms"; fi
              if [ -L "$PROJ_ROOT/data/LabeledDataset_synth/logits" ]; then echo "Symlink for logits already exists"; else ln -sf "/mnt/storage/data/LabeledDataset_synth/logits" "$PROJ_ROOT/data/LabeledDataset_synth/logits"; echo "Created symlink for logits"; fi
              if [ -L "$PROJ_ROOT/data/LabeledDataset_synth/cache" ]; then echo "Symlink for LabeledDataset_synth cache already exists"; else ln -sf "/mnt/storage/data/LabeledDataset_synth/cache" "$PROJ_ROOT/data/LabeledDataset_synth/cache"; echo "Created symlink for LabeledDataset_synth cache"; fi

              # Check file counts
              echo "---- Checking for audio files (reference) ----"
              find "/mnt/storage/data/LabeledDataset/Audio" -type f \( -name "*.mp3" -o -name "*.wav" \) | wc -l
              echo "---- Checking for REAL label files ----"
              find "/mnt/storage/data/LabeledDataset/Labels" -type f \( -name "*.lab" -o -name "*.txt" \) | wc -l
              echo "---- Checking for spectrogram files ----"
              find "/mnt/storage/data/LabeledDataset_synth/spectrograms" -type f \( -name "*.npy" -o -name "*.pt" \) | wc -l
              echo "---- Checking for logit files ----"
              find "/mnt/storage/data/LabeledDataset_synth/logits" -type f \( -name "*.npy" -o -name "*.pt" \) | wc -l

              echo "Data directory structure prepared."
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      containers:
        - name: finetune-trainer
          image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime # Consider updating PyTorch version if needed
          workingDir: /mnt/storage/ChordMini
          imagePullPolicy: IfNotPresent
          env:
            # These environment variables override defaults in student_config.yaml
            - name: PYTHONPATH
              value: /mnt/storage/ChordMini
            # Model scale - overrides student_config.yaml
            - name: MODEL_SCALE
              value: "1.0"  # Default scale: 1.0 = base model (only used for ChordNet)
            # Model type - ChordNet or BTC
            - name: MODEL_TYPE
              value: "BTC"  # Default model type: BTC
            # BTC model config path (only used if MODEL_TYPE=BTC)
            - name: BTC_CONFIG_PATH # ADDED: Path to BTC config
              value: "/mnt/storage/ChordMini/config/btc_config.yaml"
            # BTC model checkpoint path (only used if MODEL_TYPE=BTC)
            - name: BTC_CHECKPOINT
              value: "/mnt/storage/checkpoints/btc_model_best.pth" # Path to the PRETRAINED BTC model
            # ChordNet model checkpoint path (only used if MODEL_TYPE=ChordNet)
            - name: PRETRAINED_MODEL # Renamed from PRETRAINED_MODEL for clarity
              value: "/mnt/storage/checkpoints/student_model_best.pth" # Path to the PRETRAINED ChordNet model
            # Teacher model checkpoint path for loading normalization parameters
            - name: TEACHER_CHECKPOINT # Renamed from TEACHER_CHECKPOINT_FOR_NORM
              value: "/mnt/storage/BTC/test/btc_model_large_voca.pt" # Path to the teacher model for norm stats
            # Learning rate parameters
            - name: LEARNING_RATE
              value: "0.0001"  # Lower base learning rate for fine-tuning
            - name: MIN_LEARNING_RATE
              value: "0.000001"  # Minimum learning rate
            # Knowledge distillation settings (Offline KD)
            - name: USE_KD_LOSS
              value: "true"  # Enable KD loss using pre-computed logits
            - name: KD_ALPHA
              value: "0.3"  # Weight for KD loss (0-1)
            - name: TEMPERATURE
              value: "3.0"  # Temperature for softening distributions
            # Focal loss settings
            - name: USE_FOCAL_LOSS
              value: "true"  # Enable focal loss
            - name: FOCAL_GAMMA
              value: "2.0"   # Focusing parameter
            - name: FOCAL_ALPHA
              value: "0.25"   # Class weight parameter (optional)
            # Warmup settings
            - name: USE_WARMUP
              value: "true"  # Disable warm-up for fine-tuning usually
            - name: WARMUP_START_LR
              value: "0.00009"
            - name: WARMUP_END_LR
              value: "0.0001"
            - name: WARMUP_EPOCHS
              value: "5"
            # Dropout setting
            - name: DROPOUT
              value: "0.2"
            # Data paths (Using SynthDataset structure with REAL labels)
            - name: DATA_ROOT # Base data root
              value: "/mnt/storage/data"
            - name: SPECTROGRAMS_DIR # Pre-computed spectrograms
              value: "/mnt/storage/data/LabeledDataset_synth/spectrograms"
            - name: LOGITS_DIR # Pre-computed teacher logits
              value: "/mnt/storage/data/LabeledDataset_synth/logits"
            - name: LABEL_DIRS # REAL ground truth label directories (space-separated)
              # Point to the base directory; SynthDataset will handle subdirs
              value: "/mnt/storage/data/LabeledDataset/Labels"
            - name: CACHE_DIR # Cache directory for SynthDataset
              value: "/mnt/storage/data/LabeledDataset_synth/cache"
            # Model vocabulary size - MUST match pretrained model
            - name: NUM_CHORDS # Force number of classes if needed
              value: "170"
            - name: USE_VOCA # Ensure large vocabulary is used
              value: "true"
            # Fine-tuning specific settings
            - name: FREEZE_FEATURE_EXTRACTOR
              value: "false"  # Set to true to freeze early layers
            - name: EPOCHS
              value: "100"  # Number of fine-tuning epochs
            - name: BATCH_SIZE
              value: "128"  # Adjust batch size based on GPU memory
            # LR schedule
            - name: LR_SCHEDULE
              value: "validation"  # e.g., validation, cosine, none
            # GPU settings
            - name: GPU_MEMORY_FRACTION
              value: "0.95"
            - name: CUDA_VISIBLE_DEVICES
              value: "0"
            # Dataset caching settings for SynthDataset
            - name: DISABLE_CACHE
              value: "false" # Enable caching (recommended for SynthDataset)
            - name: METADATA_CACHE
              value: "false" # Cache full features if possible
            - name: LAZY_INIT
              value: "false" # Use lazy init to save startup memory
            - name: BATCH_GPU_CACHE
              value: "false" # Usually false unless specific pattern benefits
            - name: PREFETCH_FACTOR # ADDED: For DataLoader optimization
              value: "6"     # ! NO EFFECT if num_workers=0
            # Checkpoint settings
            - name: SAVE_DIR # This is the base save directory
              value: "/mnt/storage/checkpoints/finetune"
            - name: SUB_DIR # ADDED: Subdirectory for this specific run
              value: "cv" # Example: change as needed for different runs
            # Add reset epoch control option
            - name: RESET_EPOCH
              value: "true"  # Force reset epoch for fine-tuning start
            # Add reset scheduler control option
            - name: RESET_SCHEDULER
              value: "true"  # Force reset scheduler for fine-tuning start
            # Small dataset percentage for quick testing
            - name: SMALL_DATASET
              value: "1.0" # Set to e.g., "0.1" for 10% dataset, empty for full dataset
          command:
            - sh
            - -c
            - |
              set -ex
              echo "=== Fine-tuning model (SynthDataset structure with real labels) ==="

              # Install dependencies
              apt-get update && apt-get install -y libsndfile1 || { echo "ERROR: Failed to install system dependencies"; exit 1; }
              # Create minimal requirements if needed
              if [ ! -f requirements.txt ]; then
                echo "Creating minimal requirements.txt"
                echo "numpy==1.22.0" > requirements.txt
                echo "librosa>=0.8.0" >> requirements.txt
                echo "torch>=1.9.0" >> requirements.txt
                echo "tqdm>=4.62.0" >> requirements.txt
                echo "scikit-learn>=1.0.0" >> requirements.txt
                echo "pyyaml>=6.0.0" >> requirements.txt
                echo "soundfile>=0.10.0" >> requirements.txt
                echo "matplotlib>=3.0.0" >> requirements.txt # Added for plotting
                echo "seaborn>=0.11.0" >> requirements.txt # Added for plotting
              fi
              pip install --no-cache-dir -r requirements.txt || { echo "WARNING: Some requirements may have failed"; }
              # Ensure necessary packages are installed
              pip install --no-cache-dir matplotlib seaborn tqdm pyyaml librosa scikit-learn soundfile || { echo "ERROR: Failed to install additional Python packages"; exit 1; }

              # Verify data paths passed from ENV
              echo "Spectrograms Dir: ${SPECTROGRAMS_DIR}"
              echo "Logits Dir: ${LOGITS_DIR}"
              echo "Label Dirs: ${LABEL_DIRS}"
              echo "Cache Dir: ${CACHE_DIR}"
              echo "Save Dir (base): ${SAVE_DIR}"
              echo "Sub Dir: ${SUB_DIR}"

              # Construct effective save directory
              EFFECTIVE_SAVE_DIR="${SAVE_DIR}"
              if [ -n "${SUB_DIR}" ]; then
                EFFECTIVE_SAVE_DIR="${EFFECTIVE_SAVE_DIR}/${SUB_DIR}"
              fi
              echo "Effective Save Dir for this run: ${EFFECTIVE_SAVE_DIR}"

              # Create checkpoints directory
              mkdir -p "${EFFECTIVE_SAVE_DIR}"

              # Check file counts again inside container
              echo "Checking file counts inside container..."
              echo "Spectrograms: $(find "${SPECTROGRAMS_DIR}" -type f \( -name "*.npy" -o -name "*.pt" \) | wc -l)"
              echo "Logits: $(find "${LOGITS_DIR}" -type f \( -name "*.npy" -o -name "*.pt" \) | wc -l)"
              # Count labels by iterating through space-separated dirs
              LABEL_COUNT=0
              for LDIR in ${LABEL_DIRS}; do
                COUNT=$(find "${LDIR}" -type f \( -name "*.lab" -o -name "*.txt" \) | wc -l)
                echo "Labels in ${LDIR}: ${COUNT}"
                LABEL_COUNT=$((LABEL_COUNT + COUNT))
              done
              echo "Total Labels: ${LABEL_COUNT}"

              # Prepare command line arguments
              CMD_ARGS=""

              # Model Type and Pretrained Path
              CMD_ARGS="${CMD_ARGS} --model_type ${MODEL_TYPE}"
              if [ "${MODEL_TYPE}" = "BTC" ]; then
                # --- ADDED: Pass btc_config path ---
                if [ -n "${BTC_CONFIG_PATH}" ]; then
                  # This line adds the argument if BTC_CONFIG_PATH is set
                  CMD_ARGS="${CMD_ARGS} --btc_config ${BTC_CONFIG_PATH}"
                else
                  echo "WARNING: MODEL_TYPE is BTC but BTC_CONFIG_PATH is not set. Using default."
                fi
                # --- END ADDED ---
                if [ -n "${BTC_CHECKPOINT}" ]; then
                  CMD_ARGS="${CMD_ARGS} --btc_checkpoint ${BTC_CHECKPOINT}"
                else
                  echo "WARNING: MODEL_TYPE is BTC but BTC_CHECKPOINT is not set. Starting from scratch?"
                fi
              else # ChordNet
                if [ -n "${PRETRAINED_MODEL}" ]; then
                  CMD_ARGS="${CMD_ARGS} --pretrained ${PRETRAINED_MODEL}"
                else
                  echo "ERROR: MODEL_TYPE is ChordNet but PRETRAINED_MODEL is not set."
                  exit 1
                fi
              fi

              # Data Paths
              CMD_ARGS="${CMD_ARGS} --spectrograms_dir ${SPECTROGRAMS_DIR}"
              CMD_ARGS="${CMD_ARGS} --logits_dir ${LOGITS_DIR}"
              CMD_ARGS="${CMD_ARGS} --label_dirs ${LABEL_DIRS}" # Pass space-separated list directly
              CMD_ARGS="${CMD_ARGS} --cache_dir ${CACHE_DIR}"
              CMD_ARGS="${CMD_ARGS} --save_dir ${EFFECTIVE_SAVE_DIR}" # MODIFIED: Use effective save dir

              # Training Params
              CMD_ARGS="${CMD_ARGS} --learning_rate ${LEARNING_RATE}"
              CMD_ARGS="${CMD_ARGS} --min_learning_rate ${MIN_LEARNING_RATE}"
              CMD_ARGS="${CMD_ARGS} --epochs ${EPOCHS}"
              CMD_ARGS="${CMD_ARGS} --batch_size ${BATCH_SIZE}"
              CMD_ARGS="${CMD_ARGS} --lr_schedule ${LR_SCHEDULE}"
              CMD_ARGS="${CMD_ARGS} --dropout ${DROPOUT}"
              CMD_ARGS="${CMD_ARGS} --model_scale ${MODEL_SCALE}"

              # Vocabulary
              if [ "${USE_VOCA}" = "true" ]; then CMD_ARGS="${CMD_ARGS} --use_voca"; fi
              if [ -n "${NUM_CHORDS}" ]; then CMD_ARGS="${CMD_ARGS} --force_num_classes ${NUM_CHORDS}"; fi

              # KD Loss
              if [ "${USE_KD_LOSS}" = "true" ]; then
                CMD_ARGS="${CMD_ARGS} --use_kd_loss"
                if [ -n "${KD_ALPHA}" ]; then CMD_ARGS="${CMD_ARGS} --kd_alpha ${KD_ALPHA}"; fi
                if [ -n "${TEMPERATURE}" ]; then CMD_ARGS="${CMD_ARGS} --temperature ${TEMPERATURE}"; fi
              fi

              # Focal Loss
              if [ "${USE_FOCAL_LOSS}" = "true" ]; then
                CMD_ARGS="${CMD_ARGS} --use_focal_loss"
                if [ -n "${FOCAL_GAMMA}" ]; then CMD_ARGS="${CMD_ARGS} --focal_gamma ${FOCAL_GAMMA}"; fi
                if [ -n "${FOCAL_ALPHA}" ]; then CMD_ARGS="${CMD_ARGS} --focal_alpha ${FOCAL_ALPHA}"; fi
              fi

              # Warmup
              if [ "${USE_WARMUP}" = "true" ]; then
                CMD_ARGS="${CMD_ARGS} --use_warmup"
                if [ -n "${WARMUP_EPOCHS}" ]; then CMD_ARGS="${CMD_ARGS} --warmup_epochs ${WARMUP_EPOCHS}"; fi
                if [ -n "${WARMUP_START_LR}" ]; then CMD_ARGS="${CMD_ARGS} --warmup_start_lr ${WARMUP_START_LR}"; fi
                if [ -n "${WARMUP_END_LR}" ]; then CMD_ARGS="${CMD_ARGS} --warmup_end_lr ${WARMUP_END_LR}"; fi
              fi

              # Freezing
              if [ "${FREEZE_FEATURE_EXTRACTOR}" = "true" ]; then CMD_ARGS="${CMD_ARGS} --freeze_feature_extractor"; fi

              # Caching
              if [ "${DISABLE_CACHE}" = "true" ]; then CMD_ARGS="${CMD_ARGS} --disable_cache"; fi
              if [ "${METADATA_CACHE}" = "true" ]; then CMD_ARGS="${CMD_ARGS} --metadata_cache"; fi # Boolean flag, don't pass value
              if [ "${LAZY_INIT}" = "true" ]; then CMD_ARGS="${CMD_ARGS} --lazy_init"; fi # Boolean flag, don't pass value
              if [ "${BATCH_GPU_CACHE}" = "true" ]; then CMD_ARGS="${CMD_ARGS} --batch_gpu_cache"; fi # Boolean flag, don't pass value
              if [ -n "${PREFETCH_FACTOR}" ]; then CMD_ARGS="${CMD_ARGS} --prefetch_factor ${PREFETCH_FACTOR}"; fi # ADDED

              # Small Dataset
              if [ -n "${SMALL_DATASET}" ]; then
                  # Basic validation for fraction
                  IS_VALID=$(echo "${SMALL_DATASET}" | awk '{if ($1 > 0 && $1 <= 1) print "yes"; else print "no"}')
                  if [ "${IS_VALID}" = "yes" ]; then
                      echo "Using small dataset fraction: ${SMALL_DATASET}"
                      CMD_ARGS="${CMD_ARGS} --small_dataset ${SMALL_DATASET}"
                  else
                      echo "WARNING: Invalid SMALL_DATASET value '${SMALL_DATASET}'. Using full dataset."
                  fi
              fi

              # GPU Settings
              CMD_ARGS="${CMD_ARGS} --gpu_memory_fraction ${GPU_MEMORY_FRACTION}"

              # Add reset flags to command line arguments
              if [ "${RESET_EPOCH}" = "true" ]; then CMD_ARGS="${CMD_ARGS} --reset_epoch"; fi
              if [ "${RESET_SCHEDULER}" = "true" ]; then CMD_ARGS="${CMD_ARGS} --reset_scheduler"; fi

              # Add teacher checkpoint path for normalization
              if [ -n "${TEACHER_CHECKPOINT}" ]; then CMD_ARGS="${CMD_ARGS} --teacher_checkpoint ${TEACHER_CHECKPOINT}"; fi

              # Run the fine-tuning script
              echo "Running command:"
              # The ${CMD_ARGS} variable includes --btc_config when appropriate
              echo "python train_finetune.py --config ./config/student_config.yaml ${CMD_ARGS}"

              python train_finetune.py --config ./config/student_config.yaml ${CMD_ARGS}

              echo "=== Fine-tuning complete ==="

              # Clear GPU cache
              python -c "import torch; torch.cuda.empty_cache() if torch.cuda.is_available() else None"
          resources:
            requests:
              cpu: "2"
              memory: "16Gi"
              nvidia.com/gpu: "1"
            limits:
              cpu: "5"
              memory: "64Gi" # Increased memory limit
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      volumes:
        - name: storage
          persistentVolumeClaim:
            claimName: temporary-storage