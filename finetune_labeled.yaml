apiVersion: batch/v1
kind: Job
metadata:
  name: chord-finetune-labeled
  namespace: csuf-titans
  labels:
    app: chord-finetune
spec:
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: chord-finetune
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - NVIDIA-GeForce-RTX-3090
      restartPolicy: Never
      tolerations:
        - key: "node.kubernetes.io/not-ready"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
        - key: "node.kubernetes.io/unreachable"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
      initContainers:
        - name: clone-repo
          image: alpine:3.16
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              set -ex
              echo "Installing git..."
              apk update && apk add --no-cache git
              echo "Checking for existing project repository..."
              if [ ! -d "/mnt/storage/ChordMini" ]; then
                echo "Cloning repository..."
                git clone https://gitlab.com/ptnghia-j/ChordMini.git /mnt/storage/ChordMini || {
                  echo "ERROR: Failed to clone repository"
                  exit 1
                }
              else
                echo "Repository already exists."
              fi
              exit 0
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
        - name: data-check
          image: alpine:3.16
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              set -ex
              echo "Checking data directory structure..."

              # Locate the project root
              PROJ_ROOT="/mnt/storage/ChordMini"
              echo "Project root: $PROJ_ROOT"

              # Check and create main data directories
              echo "Checking/creating data directories..."
              for dir in "/mnt/storage/data/LabeledDataset/Audio" "/mnt/storage/data/LabeledDataset/Labels" "/mnt/storage/data/LabeledDataset/cache"; do
                if [ -d "$dir" ]; then
                  echo "Directory already exists: $dir"
                else
                  mkdir -p "$dir"
                  echo "Created directory: $dir"
                fi
              done

              # Check and create audio dataset subdirectories
              echo "Checking/creating audio dataset subdirectories..."
              for subdir in "billboard" "caroleKing" "queen" "theBeatles"; do
                dir="/mnt/storage/data/LabeledDataset/Audio/$subdir"
                if [ -d "$dir" ]; then
                  echo "Audio directory already exists: $subdir"
                else
                  mkdir -p "$dir"
                  echo "Created audio directory: $subdir"
                fi
              done

              # Check and create label dataset subdirectories
              echo "Checking/creating label dataset subdirectories..."
              for subdir in "billboardLabels" "caroleKingLabels" "queenLabels" "theBeatlesLabels"; do
                dir="/mnt/storage/data/LabeledDataset/Labels/$subdir"
                if [ -d "$dir" ]; then
                  echo "Label directory already exists: $subdir"
                else
                  mkdir -p "$dir"
                  echo "Created label directory: $subdir"
                fi
              done

              # Create symlinks from project directory to actual data location
              mkdir -p "$PROJ_ROOT/data/LabeledDataset"

              # Create unified symlinks for LabeledDataset
              if [ -L "$PROJ_ROOT/data/LabeledDataset/Audio" ]; then
                echo "Symlink for Audio already exists"
              else
                ln -sf "/mnt/storage/data/LabeledDataset/Audio" "$PROJ_ROOT/data/LabeledDataset/Audio"
                echo "Created symlink for Audio"
              fi

              if [ -L "$PROJ_ROOT/data/LabeledDataset/Labels" ]; then
                echo "Symlink for Labels already exists"
              else
                ln -sf "/mnt/storage/data/LabeledDataset/Labels" "$PROJ_ROOT/data/LabeledDataset/Labels"
                echo "Created symlink for Labels"
              fi

              if [ -L "$PROJ_ROOT/data/LabeledDataset/cache" ]; then
                echo "Symlink for cache already exists"
              else
                ln -sf "/mnt/storage/data/LabeledDataset/cache" "$PROJ_ROOT/data/LabeledDataset/cache"
                echo "Created symlink for cache"
              fi

              # Check for audio files
              echo "---- Checking for audio files ----"
              find "/mnt/storage/data/LabeledDataset/Audio" -name "*.mp3" -o -name "*.wav" | wc -l

              # Check for label files
              echo "---- Checking for label files ----"
              find "/mnt/storage/data/LabeledDataset/Labels" -name "*.lab" -o -name "*.txt" | wc -l

              echo "Data directory structure prepared."
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      containers:
        - name: finetune-trainer
          image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime
          workingDir: /mnt/storage/ChordMini
          imagePullPolicy: IfNotPresent
          env:
            # These environment variables override defaults in student_config.yaml
            - name: PYTHONPATH
              value: /mnt/storage/ChordMini
            # Model scale - overrides student_config.yaml
            - name: MODEL_SCALE
              value: "1.0"  # Default scale: 1.0 = base model
            # Model type - ChordNet or BTC
            - name: MODEL_TYPE
              value: "ChordNet"  # Default model type: ChordNet
            # BTC model checkpoint path (only used if MODEL_TYPE=BTC)
            - name: BTC_CHECKPOINT
              value: "/mnt/storage/checkpoints/btc/btc_model_best.pth"
            # Learning rate parameters
            - name: LEARNING_RATE
              value: "0.00001"  # Lower base learning rate for fine-tuning
            - name: MIN_LEARNING_RATE
              value: "0.000001"  # Minimum learning rate
            # Knowledge distillation settings
            # -------------------------------------------------------------------------
            # Knowledge Distillation (KD) allows the model to learn from a teacher model
            # The teacher model is typically a larger, more powerful model that has been
            # pre-trained on the same or similar task.
            # -------------------------------------------------------------------------
            - name: USE_KD_LOSS
              value: "false"  # Set to "true" to enable KD loss
            # Add explicit note about KD status
            - name: KD_STATUS_CHECK
              value: "disabled_explicitly"  # For debugging purposes
            # Teacher model path - if not specified, will try to find btc_model_large_voca.pt
            - name: TEACHER_MODEL
              value: "/mnt/storage/checkpoints/btc/btc_model_large_voca.pt"
            # KD loss weight - controls balance between KD loss and standard loss
            # Higher values give more weight to matching teacher outputs vs ground truth
            - name: KD_ALPHA
              value: "0.3"  # Weight for KD loss (0-1) - only used if KD is enabled
            # Temperature for softening probability distributions
            # Higher values make distributions softer, emphasizing knowledge transfer
            - name: TEMPERATURE
              value: "2.0"  # Temperature for softening distributions
            # Debug mode for teacher logit extraction - saves intermediate tensors
            - name: KD_DEBUG_MODE
              value: "true"  # Set to "true" to enable debug mode for teacher logit extraction
            # Focal loss settings
            - name: USE_FOCAL_LOSS
              value: "true"  # Set to "true" to enable focal loss
            - name: FOCAL_GAMMA
              value: "2.0"   # Focusing parameter
            - name: FOCAL_ALPHA
              value: "0.25"   # Class weight parameter
            # Warmup settings
            - name: USE_WARMUP
              value: "false"  # Set to "true" to enable learning rate warm-up
            - name: WARMUP_START_LR
              value: "0.00001"  # Starting learning rate for warm-up
            - name: WARMUP_END_LR
              value: "0.00004"  # Final learning rate after warm-up
            - name: WARMUP_EPOCHS
              value: "4"  # Number of epochs for warm-up
            # Dropout setting
            - name: DROPOUT
              value: "0.4"  # Set dropout probability (0-1)
            # Data paths
            - name: DATA_ROOT
              value: "/mnt/storage/data"
            # Model vocabulary size - crucial for matching pretrained model
            - name: NUM_CHORDS
              value: "170"  # Must match pretrained model's 170 chord classes
            - name: USE_VOCA
              value: "true"  # Use large vocabulary (170 chords)
            # Fine-tuning specific settings
            - name: PRETRAINED_MODEL
              value: "/mnt/storage/checkpoints/student/student_model_best.pth"  # Path to pretrained model
            - name: FREEZE_FEATURE_EXTRACTOR
              value: "false"  # Whether to freeze feature extraction layers
            - name: EPOCHS
              value: "100"  # Number of fine-tuning epochs
            - name: BATCH_SIZE
              value: "32"  # Batch size for training
            # LR schedule
            - name: LR_SCHEDULE
              value: "validation"  # LR schedule type
            # GPU settings
            - name: GPU_MEMORY_FRACTION
              value: "0.95"  # Use 95% of available GPU memory
            - name: CUDA_VISIBLE_DEVICES
              value: "0"
            # Dataset caching settings
            - name: DISABLE_CACHE
              value: "false"  # Use caching for features
            - name: METADATA_CACHE
              value: "false"  # Don't use metadata-only caching
            # Checkpoint settings
            - name: SAVE_DIR
              value: "/mnt/storage/checkpoints/finetune"  # Directory to save checkpoints
            # Cross-validation settings
            - name: USE_CROSS_VALIDATION
              value: "false"  # Set to "true" to enable cross-validation
            - name: KFOLD
              value: "0"      # Which fold to use for validation (0-4)
            - name: TOTAL_FOLDS
              value: "5"      # Total number of folds
            # Total number of folds
            - name: PREPROCESS
              value: "false"  # Set to "true" to preprocess and generate features
            # Small dataset percentage for quick testing
            - name: SMALL_DATASET
              value: ""  # Set to a value like "0.1" to use only 10% of the dataset
          command:
            - sh
            - -c
            - |
              set -ex
              echo "=== Fine-tuning model on labeled data ==="

              # Install necessary system packages
              apt-get update && apt-get install -y libsndfile1 || { echo "ERROR: Failed to install system dependencies"; exit 1; }

              # Create a minimal requirements.txt if missing
              if [ ! -f requirements.txt ]; then
                echo "Creating minimal requirements.txt"
                echo "numpy==1.22.0" > requirements.txt
                echo "librosa>=0.8.0" >> requirements.txt
                echo "torch>=1.9.0" >> requirements.txt
                echo "tqdm>=4.62.0" >> requirements.txt
                echo "scikit-learn>=1.0.0" >> requirements.txt
                echo "pyyaml>=6.0.0" >> requirements.txt  # Added for config loading
                echo "soundfile>=0.10.0" >> requirements.txt  # For audio file loading
              fi

              pip install --no-cache-dir -r requirements.txt || { echo "WARNING: Some requirements may have failed"; }
              pip install --no-cache-dir matplotlib tqdm pyyaml librosa scikit-learn soundfile || { echo "ERROR: Failed to install additional Python packages"; exit 1; }

              # Verify the data paths
              echo "Checking LabeledDataset paths..."

              # Create checkpoints directory
              mkdir -p ${SAVE_DIR}

              # Check for audio/label files
              echo "Checking for audio files..."
              find ${DATA_ROOT}/LabeledDataset/Audio -type f | grep -E '\.(mp3|wav)$' | wc -l

              echo "Checking for label files..."
              find ${DATA_ROOT}/LabeledDataset/Labels -type f | grep -E '\.(lab|txt)$' | wc -l

              # Prepare command line arguments
              KD_ARGS=""
              if [ "${USE_KD_LOSS}" = "true" ]; then
                KD_ARGS="--use_kd_loss --kd_alpha ${KD_ALPHA} --temperature ${TEMPERATURE}"
                # Add teacher model path if specified
                if [ -n "${TEACHER_MODEL}" ]; then
                  KD_ARGS="${KD_ARGS} --teacher_model ${TEACHER_MODEL}"
                fi
                # Add debug mode if enabled
                if [ "${KD_DEBUG_MODE}" = "true" ]; then
                  KD_ARGS="${KD_ARGS} --kd_debug_mode"
                fi
              fi

              FOCAL_ARGS=""
              if [ "${USE_FOCAL_LOSS}" = "true" ]; then
                FOCAL_ARGS="--use_focal_loss --focal_gamma ${FOCAL_GAMMA}"
                if [ "${FOCAL_ALPHA}" != "" ]; then
                  FOCAL_ARGS="${FOCAL_ARGS} --focal_alpha ${FOCAL_ALPHA}"
                fi
              fi

              WARMUP_ARGS=""
              if [ "${USE_WARMUP}" = "true" ]; then
                WARMUP_ARGS="--use_warmup --warmup_epochs ${WARMUP_EPOCHS} --warmup_start_lr ${WARMUP_START_LR} --warmup_end_lr ${WARMUP_END_LR}"
              fi

              FREEZE_ARGS=""
              if [ "${FREEZE_FEATURE_EXTRACTOR}" = "true" ]; then
                FREEZE_ARGS="--freeze_feature_extractor"
              fi

              CACHE_ARGS=""
              if [ "${DISABLE_CACHE}" = "true" ]; then
                CACHE_ARGS="--disable_cache"
              fi

              if [ "${METADATA_CACHE}" = "true" ]; then
                CACHE_ARGS="${CACHE_ARGS} --metadata_cache"
              fi

              # Define directories
              AUDIO_DIRS="${DATA_ROOT}/LabeledDataset/Audio/billboard ${DATA_ROOT}/LabeledDataset/Audio/caroleKing ${DATA_ROOT}/LabeledDataset/Audio/queen ${DATA_ROOT}/LabeledDataset/Audio/theBeatles"
              LABEL_DIRS="${DATA_ROOT}/LabeledDataset/Labels/billboardLabels ${DATA_ROOT}/LabeledDataset/Labels/caroleKingLabels ${DATA_ROOT}/LabeledDataset/Labels/queenLabels ${DATA_ROOT}/LabeledDataset/Labels/theBeatlesLabels"
              CACHE_DIR="${DATA_ROOT}/LabeledDataset/cache"

              # Cross-validation arguments
              CV_ARGS=""
              if [ "${USE_CROSS_VALIDATION}" = "true" ]; then
                echo "Using cross-validation with fold ${KFOLD} of ${TOTAL_FOLDS}"
                if [ "${PREPROCESS}" = "true" ]; then
                  # Run preprocessing first if requested
                  echo "Running preprocessing step to generate features..."
                  python train_cv_kd.py --config ./config/student_config.yaml \
                    --kfold ${KFOLD} \
                    --total_folds ${TOTAL_FOLDS} \
                    --audio_dirs ${AUDIO_DIRS} \
                    --label_dirs ${LABEL_DIRS} \
                    --cache_dir ${CACHE_DIR} \
                    --preprocess

                  echo "Preprocessing completed."
                fi

                # Use cross-validation training script instead of standard fine-tuning
                echo "Starting cross-validation training with fold ${KFOLD}..."
                python train_cv_kd.py --config ./config/student_config.yaml \
                  --kfold ${KFOLD} \
                  --total_folds ${TOTAL_FOLDS} \
                  --save_dir ${SAVE_DIR}/cv_fold${KFOLD} \
                  --audio_dirs ${AUDIO_DIRS} \
                  --label_dirs ${LABEL_DIRS} \
                  --cache_dir ${CACHE_DIR} \
                  --learning_rate ${LEARNING_RATE} \
                  --batch_size ${BATCH_SIZE} \
                  ${KD_ARGS}
              else
                # Add small dataset argument if specified
                SMALL_DATASET_ARG=""
                if [ "${SMALL_DATASET}" != "" ]; then
                  # Validate that SMALL_DATASET is a number between 0 and 1
                  if [[ "${SMALL_DATASET}" =~ ^0*([.][0-9]+|1[.]0*|0|1)$ ]]; then
                    echo "Using only ${SMALL_DATASET} fraction of dataset for quick testing"
                    SMALL_DATASET_ARG="--small_dataset ${SMALL_DATASET}"
                  else
                    echo "WARNING: SMALL_DATASET value '${SMALL_DATASET}' is not a valid fraction between 0 and 1. Ignoring."
                  fi
                fi

                # Run the standard fine-tuning script with all options
                echo "Starting fine-tuning with all options..."

                # Add model type arguments
                MODEL_TYPE_ARGS="--model_type ${MODEL_TYPE}"

                # Add BTC checkpoint if model type is BTC
                if [ "${MODEL_TYPE}" = "BTC" ]; then
                  MODEL_TYPE_ARGS="${MODEL_TYPE_ARGS} --btc_checkpoint ${BTC_CHECKPOINT}"
                  echo "Using BTC model with checkpoint: ${BTC_CHECKPOINT}"
                else
                  echo "Using ChordNet model"
                fi

                python train_finetune.py --config ./config/student_config.yaml \
                  --pretrained ${PRETRAINED_MODEL} \
                  --use_voca \
                  --save_dir ${SAVE_DIR} \
                  --audio_dirs ${AUDIO_DIRS} \
                  --label_dirs ${LABEL_DIRS} \
                  --cache_dir ${CACHE_DIR} \
                  --learning_rate ${LEARNING_RATE} \
                  --min_learning_rate ${MIN_LEARNING_RATE} \
                  --model_scale ${MODEL_SCALE} \
                  --dropout ${DROPOUT} \
                  --epochs ${EPOCHS} \
                  --batch_size ${BATCH_SIZE} \
                  --lr_schedule ${LR_SCHEDULE} \
                  --gpu_memory_fraction ${GPU_MEMORY_FRACTION} \
                  ${MODEL_TYPE_ARGS} ${WARMUP_ARGS} ${KD_ARGS} ${FOCAL_ARGS} ${FREEZE_ARGS} ${CACHE_ARGS} ${SMALL_DATASET_ARG}
              fi

              echo "=== Training complete ==="

              # Clear GPU cache to release memory before exiting
              python -c "import torch; torch.cuda.empty_cache() if torch.cuda.is_available() else None"
          resources:
            requests:
              cpu: "2"
              memory: "16Gi"
              nvidia.com/gpu: "1"
            limits:
              cpu: "5"
              memory: "24Gi"
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      volumes:
        - name: storage
          persistentVolumeClaim:
            claimName: temporary-storage