apiVersion: batch/v1
kind: Job
metadata:
  name: chord-cv-kd-0.1
  namespace: csuf-titans
  labels:
    app: chord-cv-kd-training
spec:
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: chord-cv-kd-training
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - NVIDIA-A10 # Or other suitable GPU
              - key: kubernetes.io/hostname
                operator: In
                values:
                - gpu-13.nrp.mghpcc.org
      restartPolicy: Never
      tolerations:
        - key: "node.kubernetes.io/not-ready"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
        - key: "node.kubernetes.io/unreachable"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
      initContainers:
        - name: clone-repo
          image: alpine:3.16
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              set -ex
              echo "Installing git..."
              apk update && apk add --no-cache git
              echo "Checking for existing project repository..."
              if [ ! -d "/mnt/storage/ChordMini" ]; then
                echo "Cloning repository..."
                git clone https://gitlab.com/ptnghia-j/ChordMini.git /mnt/storage/ChordMini || {
                  echo "ERROR: Failed to clone repository"
                  exit 1
                }
              else
                echo "Repository already exists."
              fi
              exit 0
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      containers:
        - name: cv-kd-trainer
          image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime
          workingDir: /mnt/storage/ChordMini
          imagePullPolicy: IfNotPresent
          env:
            # These environment variables override defaults in student_config.yaml
            - name: PYTHONPATH
              value: /mnt/storage/ChordMini
            # Model settings
            - name: MODEL_SCALE
              value: "1.0"  # Scale factor for model size (1.0 = standard, 2.0 = 2x larger)
            - name: MODEL_TYPE
              value: "BTC"  # Model type: ChordNet or BTC
            - name: INITIAL_MODEL_CKPT_PATH
              value: "/mnt/storage/checkpoints/btc_model_best.pth"  # Path to initial model checkpoint (works for both BTC and ChordNet)
            # Learning rate scheduling options
            - name: LR_SCHEDULE
              value: "validation"  # Options: cosine, step, validation
            # Additional learning rate parameters
            - name: LEARNING_RATE
              value: "0.0001"  # Base learning rate
            - name: MIN_LEARNING_RATE
              value: "0.000001"  # Minimum learning rate for schedulers
            # Knowledge distillation settings
            - name: USE_KD_LOSS
              value: "true"  # Set to "true" to enable KD loss
            - name: KD_ALPHA
              value: "0.1"  # Weight for KD loss (0-1)
            - name: TEMPERATURE
              value: "3.0"  # Temperature for softening distributions
            - name: TEACHER_CHECKPOINT # MODIFIED: Was TEACHER_MODEL
              value: "/mnt/storage/BTC/test/btc_model_large_voca.pt"  # Path to teacher model for normalization stats
            # Focal loss settings
            - name: USE_FOCAL_LOSS
              value: "false"  # Set to "true" to enable focal loss
            - name: FOCAL_GAMMA
              value: "2.0"   # Focusing parameter for focal loss
            - name: FOCAL_ALPHA
              value: "0.5"  # Class weight parameter for focal loss
            # Warmup settings
            - name: USE_WARMUP
              value: "true"  # Set to "true" to enable learning rate warm-up
            - name: WARMUP_START_LR
              value: "0.00009"  # Starting learning rate for warm-up
            - name: WARMUP_END_LR
              value: "0.0001"  # Final learning rate after warm-up
            - name: WARMUP_EPOCHS
              value: "4"  # Number of epochs for warm-up
            # Dropout setting
            - name: DROPOUT
              value: "0.2"  # Set dropout probability (0-1)
            # Cross-validation settings
            - name: KFOLD
              value: "2"  # Specify the fold to run (e.g., "0", "1", "2", "3", "4")
            - name: TOTAL_FOLDS
              value: "5"  # Total number of folds (should match 'completions')
            # Single data root for all data types
            - name: DATA_ROOT
              value: "/mnt/storage/data"
            # ---- BEGIN: Ensure these data path ENV VARS are defined in your YAML ----
            # These are examples, adjust paths as needed.
            # The python script (train_cv_kd.py) will pick these up via its os.environ.get() logic.
            - name: SPECTROGRAMS_DIR # ADDED: Example, ensure this is set
              value: "LabeledDataset_synth/spectrograms" # Relative to DATA_ROOT or absolute
            - name: LOGITS_DIR # ADDED: Example, ensure this is set
              value: "LabeledDataset_synth/logits" # Relative to DATA_ROOT or absolute
            - name: LABEL_DIR # ADDED: Example, ensure this is set
              value: "LabeledDataset/Labels" # Relative to DATA_ROOT or absolute
            - name: CACHE_DIR # ADDED: Example, ensure this is set
              value: "data_cache/cv_kd" # Relative to DATA_ROOT or absolute
            # ---- END: Ensure these data path ENV VARS are defined in your YAML ----
            # GPU memory utilization optimization
            - name: GPU_MEMORY_FRACTION
              value: "0.95"  # Use 95% of available GPU memory
            - name: CUDA_VISIBLE_DEVICES
              value: "0"
            # Dataset GPU optimization flags
            - name: BATCH_GPU_CACHE
              value: "true"  # Enable GPU batch caching
            - name: PREFETCH_FACTOR
              value: "6"     # ! NO EFFECT if num_workers=0
            # Add checkpoint loading control option
            - name: LOAD_CHECKPOINT
              value: "auto"  # Options: "auto" (load if exists), "never" (always start fresh), "required" (must exist), or actual path
            - name: SUB_DIR
              value: "cv-kd-fold-0.1-2"  # Make SUB_DIR specific to the KFOLD value above
            # Add reset epoch control option
            - name: RESET_EPOCH
              value: "true"  # Whether to reset epoch counter when loading a checkpoint (true/false)
                             # When true: Training will start from epoch 1 regardless of the checkpoint's epoch
                             # When false: Training will resume from the checkpoint's epoch + 1
            # Add reset scheduler control option
            - name: RESET_SCHEDULER
              value: "true"  # Whether to reset LR scheduler when loading a checkpoint (true/false)
                             # When true: Scheduler state will be reset even if RESET_EPOCH=false
                             # When false: Scheduler state will be loaded from checkpoint if available
            # Dataset initialization controls
            - name: LAZY_INIT
              value: "false"  # Controls dataset lazy initialization (true/false)
            - name: SMALL_DATASET
              value: "1.0"  # Set percentage of dataset to use (0.1 = 10%, null = full dataset)
            - name: METADATA_CACHE
              value: "false"  # Use metadata-only caching to reduce memory usage
            # ---- BEGIN: Ensure these ENV VARS are defined in your YAML ----
            - name: EPOCHS # ADDED: Example, ensure this is set
              value: "100"
            - name: BATCH_SIZE # ADDED: Example, ensure this is set
              value: "64"
            # ---- END: Ensure these ENV VARS are defined in your YAML ----
          command:
            - sh
            - -c
            - |
              set -ex
              echo "=== Cross-validation with Knowledge Distillation training container started ==="

              # Install necessary system packages
              apt-get update && apt-get install -y libsndfile1 || { echo "ERROR: Failed to install system dependencies"; exit 1; }

              # Create a minimal requirements.txt if missing
              if [ ! -f requirements.txt ]; then
                echo "Creating minimal requirements.txt"
                echo "numpy==1.22.0" > requirements.txt
                echo "librosa>=0.8.0" >> requirements.txt
                echo "torch>=1.9.0" >> requirements.txt
                echo "tqdm>=4.62.0" >> requirements.txt
                echo "scikit-learn>=1.0.0" >> requirements.txt
                echo "pyyaml>=6.0.0" >> requirements.txt  # Added for config loading
              fi

              pip install --no-cache-dir -r requirements.txt || { echo "WARNING: Some requirements may have failed"; }
              pip install --no-cache-dir matplotlib tqdm pyyaml librosa scikit-learn || { echo "ERROR: Failed to install additional Python packages"; exit 1; }

              # Get LR schedule option from environment
              LR_SCHEDULE_ARG=""
              if [ "${LR_SCHEDULE}" != "" ] && [ "${LR_SCHEDULE}" != "null" ]; then
                echo "Using ${LR_SCHEDULE} learning rate schedule"
                LR_SCHEDULE_ARG="--lr_schedule ${LR_SCHEDULE}"
              else
                echo "Using default validation-based learning rate adjustment"
              fi

              # Create separate warmup argument variable that's always set when warmup is enabled
              WARMUP_ARG=""
              if [ "${USE_WARMUP}" = "true" ]; then
                echo "Using warm-up learning rate schedule for ${WARMUP_EPOCHS} epochs"
                WARMUP_ARG="--use_warmup --warmup_epochs ${WARMUP_EPOCHS}"

                # Add the other warmup parameters to ensure they're passed to the script
                if [ "${WARMUP_START_LR}" != "" ]; then
                  WARMUP_ARG="${WARMUP_ARG} --warmup_start_lr ${WARMUP_START_LR}"
                  echo "Using warmup start learning rate: ${WARMUP_START_LR}"
                fi

                if [ "${WARMUP_END_LR}" != "" ]; then
                  WARMUP_ARG="${WARMUP_ARG} --warmup_end_lr ${WARMUP_END_LR}"
                  echo "Using warmup end learning rate: ${WARMUP_END_LR}"
                fi
              fi

              # Get learning rate arguments
              LR_ARGS=""
              if [ "${LEARNING_RATE}" != "" ]; then
                echo "Using base learning rate: ${LEARNING_RATE}"
                LR_ARGS="${LR_ARGS} --learning_rate ${LEARNING_RATE}"
              fi

              if [ "${MIN_LEARNING_RATE}" != "" ]; then
                echo "Using minimum learning rate: ${MIN_LEARNING_RATE}"
                LR_ARGS="${LR_ARGS} --min_learning_rate ${MIN_LEARNING_RATE}"
              fi

              # Get focal loss arguments
              FOCAL_LOSS_ARG=""
              if [ "${USE_FOCAL_LOSS}" = "true" ]; then
                echo "Using focal loss with gamma=${FOCAL_GAMMA}"
                FOCAL_LOSS_ARG="--use_focal_loss --focal_gamma ${FOCAL_GAMMA}"
                # Add focal_alpha if specified
                if [ "${FOCAL_ALPHA}" != "" ]; then
                  FOCAL_LOSS_ARG="${FOCAL_LOSS_ARG} --focal_alpha ${FOCAL_ALPHA}"
                  echo "Using focal loss alpha=${FOCAL_ALPHA}"
                fi
              else
                echo "Using standard cross-entropy loss (focal loss disabled)"
              fi

              # Get knowledge distillation arguments
              KD_LOSS_ARG=""
              if [ "${USE_KD_LOSS}" = "true" ]; then
                echo "Using knowledge distillation with alpha=${KD_ALPHA} and temperature=${TEMPERATURE}"
                # REMOVED: --teacher_model from here, as train_cv_kd.py uses logits_dir for offline KD
                KD_LOSS_ARG="--use_kd_loss --kd_alpha ${KD_ALPHA} --temperature ${TEMPERATURE}"
              fi

              TEACHER_CHECKPOINT_ARG=""
              if [ "${TEACHER_CHECKPOINT}" != "" ]; then
                echo "Using teacher checkpoint for normalization: ${TEACHER_CHECKPOINT}"
                TEACHER_CHECKPOINT_ARG="--teacher_checkpoint ${TEACHER_CHECKPOINT}"
              fi

              # Get model scale from environment
              MODEL_SCALE_ARG=""
              if [ "${MODEL_SCALE}" != "" ] && [ "${MODEL_SCALE}" != "1.0" ]; then
                echo "Using model scale factor: ${MODEL_SCALE}"
                MODEL_SCALE_ARG="--model_scale ${MODEL_SCALE}"
              else
                echo "Using default model scale (1.0)"
              fi

              # Get model type arguments
              MODEL_TYPE_ARGS=""
              if [ "${MODEL_TYPE}" != "" ]; then
                echo "Using model type: ${MODEL_TYPE}"
                MODEL_TYPE_ARGS="--model_type ${MODEL_TYPE}"
              fi

              # Add initial model checkpoint path if provided (for both BTC and ChordNet)
              INITIAL_MODEL_ARGS=""
              if [ "${INITIAL_MODEL_CKPT_PATH}" != "" ]; then
                echo "Using initial model checkpoint: ${INITIAL_MODEL_CKPT_PATH}"
                INITIAL_MODEL_ARGS="--initial_model_ckpt_path ${INITIAL_MODEL_CKPT_PATH}"
              fi

              # Get cross-validation arguments
              CV_ARGS=""
              if [ "${KFOLD}" != "" ]; then
                echo "Using fold ${KFOLD} for validation"
                CV_ARGS="--kfold ${KFOLD}"
              fi

              if [ "${TOTAL_FOLDS}" != "" ]; then
                echo "Using ${TOTAL_FOLDS} total folds"
                CV_ARGS="${CV_ARGS} --total_folds ${TOTAL_FOLDS}"
              fi

              DROPOUT_ARG=""
              if [ "${DROPOUT}" != "" ]; then
                echo "Using dropout: ${DROPOUT}"
                DROPOUT_ARG="--dropout ${DROPOUT}"
              fi

              EPOCHS_ARG=""
              if [ "${EPOCHS}" != "" ]; then
                echo "Setting epochs: ${EPOCHS}"
                EPOCHS_ARG="--epochs ${EPOCHS}"
              fi

              BATCH_SIZE_ARG=""
              if [ "${BATCH_SIZE}" != "" ]; then
                echo "Setting batch size: ${BATCH_SIZE}"
                BATCH_SIZE_ARG="--batch_size ${BATCH_SIZE}"
              fi

              RESET_EPOCH_ARG=""
              if [ "${RESET_EPOCH}" = "true" ]; then
                echo "Reset epoch enabled"
                RESET_EPOCH_ARG="--reset_epoch"
              fi

              RESET_SCHEDULER_ARG=""
              if [ "${RESET_SCHEDULER}" = "true" ]; then
                echo "Reset scheduler enabled"
                RESET_SCHEDULER_ARG="--reset_scheduler"
              fi

              LOAD_CHECKPOINT_ARG=""
              # Pass --load_checkpoint only if LOAD_CHECKPOINT is not empty and not "never"
              # The python script's ENV override will handle LOAD_CHECKPOINT="never" by setting args.load_checkpoint to "never"
              # The python script should ideally interpret "never" as None or skip loading.
              # For safety, we only pass the arg if it's a potential path.
              if [ "${LOAD_CHECKPOINT}" != "" ] && [ "${LOAD_CHECKPOINT}" != "never" ]; then
                echo "Attempting to load checkpoint from: ${LOAD_CHECKPOINT}"
                LOAD_CHECKPOINT_ARG="--load_checkpoint ${LOAD_CHECKPOINT}"
              elif [ "${LOAD_CHECKPOINT}" = "never" ]; then
                echo "LOAD_CHECKPOINT is 'never', will not pass --load_checkpoint arg directly, relying on ENV override in script."
                # If script doesn't handle "never" from ENV, this might still try to load a file named "never".
                # A more robust python script would check `if args.load_checkpoint == "never": args.load_checkpoint = None`
              fi

              CACHE_DIR_ARG=""
              if [ "${CACHE_DIR}" != "" ]; then
                echo "Using cache directory: ${CACHE_DIR}"
                CACHE_DIR_ARG="--cache_dir ${CACHE_DIR}"
              fi

              # Handle caching options for memory optimization
              CACHE_ARGS=""
              if [ "${METADATA_CACHE}" = "true" ]; then
                echo "Using metadata-only caching to reduce memory usage"
                CACHE_ARGS="--metadata_cache"
              else
                echo "Dataset caching disabled to reduce memory usage"
                CACHE_ARGS="--disable_cache"
              fi

              # GPU acceleration options
              GPU_ARGS=""
              if [ "${GPU_MEMORY_FRACTION}" != "" ]; then
                GPU_ARGS="--gpu_memory_fraction ${GPU_MEMORY_FRACTION}"
              fi

              if [ "${BATCH_GPU_CACHE}" = "true" ]; then
                GPU_ARGS="${GPU_ARGS} --batch_gpu_cache"
              fi

              if [ "${PREFETCH_FACTOR}" != "" ]; then
                GPU_ARGS="${GPU_ARGS} --prefetch_factor ${PREFETCH_FACTOR}"
              fi

              # Dataset initialization control arguments
              DATASET_INIT_ARGS=""
              if [ "${LAZY_INIT}" = "true" ]; then
                echo "Using lazy initialization for dataset"
                DATASET_INIT_ARGS="--lazy_init"
              else
                echo "Disabling lazy initialization for dataset (will pre-load all metadata)"
              fi

              # Small dataset argument
              SMALL_DATASET_ARG=""
              if [ "${SMALL_DATASET}" != "" ]; then
                echo "Using small dataset fraction: ${SMALL_DATASET}"
                SMALL_DATASET_ARG="--small_dataset ${SMALL_DATASET}"
              fi

              # Set up save directory
              SAVE_DIR="/mnt/storage/checkpoints/cv_kd"
              if [ "${SUB_DIR}" != "" ]; then
                SAVE_DIR="${SAVE_DIR}/${SUB_DIR}"
              fi
              echo "Using save directory: ${SAVE_DIR}"
              mkdir -p "${SAVE_DIR}"

              # Run the cross-validation with knowledge distillation script
              echo "Starting cross-validation with knowledge distillation training..."
              python train_cv_kd.py --config ./config/student_config.yaml \
                --save_dir ${SAVE_DIR} \
                --use_voca \
                ${MODEL_TYPE_ARGS} \
                ${INITIAL_MODEL_ARGS} \
                ${MODEL_SCALE_ARG} \
                ${LR_ARGS} \
                ${LR_SCHEDULE_ARG} \
                ${WARMUP_ARG} \
                ${FOCAL_LOSS_ARG} \
                ${KD_LOSS_ARG} \
                ${TEACHER_CHECKPOINT_ARG} \
                ${CV_ARGS} \
                ${GPU_ARGS} \
                ${CACHE_ARGS} \
                ${CACHE_DIR_ARG} \
                ${DATASET_INIT_ARGS} \
                ${SMALL_DATASET_ARG} \
                ${DROPOUT_ARG} \
                ${EPOCHS_ARG} \
                ${BATCH_SIZE_ARG} \
                ${RESET_EPOCH_ARG} \
                ${RESET_SCHEDULER_ARG} \
                ${LOAD_CHECKPOINT_ARG}
          resources:
            limits:
              nvidia.com/gpu: "1"
              memory: "64Gi"
              cpu: "6"
            requests:
              nvidia.com/gpu: "1"
              memory: "16Gi"
              cpu: "2"
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      volumes:
        - name: storage
          persistentVolumeClaim:
            claimName: temporary-storage