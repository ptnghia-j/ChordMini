apiVersion: batch/v1
kind: Job
metadata:
  name: chord-mini-large
  namespace: csuf-titans
  labels:
    app: chord-student-training
spec:
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: chord-student-training
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - NVIDIA-GeForce-RTX-3090
      restartPolicy: Never
      tolerations:
        - key: "node.kubernetes.io/not-ready"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
        - key: "node.kubernetes.io/unreachable"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
      initContainers:
        - name: clone-repo
          image: alpine:3.16
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              set -ex
              echo "Installing git..."
              apk update && apk add --no-cache git
              echo "Checking for existing project repository..."
              if [ ! -d "/mnt/storage/ChordMini" ]; then
                echo "Cloning repository..."
                git clone https://gitlab.com/ptnghia-j/ChordMini.git /mnt/storage/ChordMini || {
                  echo "ERROR: Failed to clone repository"
                  exit 1
                }
              else
                echo "Repository already exists."
              fi
              exit 0
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
        - name: data-check
          image: alpine:3.16
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              set -ex
              echo "Checking data directory structure..."
              
              # Locate the spectrogram and label directories
              PROJ_ROOT="/mnt/storage/ChordMini"
              echo "Project root: $PROJ_ROOT"
              
              # Create the data directories if they don't exist for both dataset types
              # FMA dataset directories
              mkdir -p "/mnt/storage/data/synth/spectrograms"
              mkdir -p "/mnt/storage/data/synth/labels"
              mkdir -p "/mnt/storage/data/synth/logits"
              
              # Maestro dataset directories
              mkdir -p "/mnt/storage/data/maestro_synth/spectrograms"
              mkdir -p "/mnt/storage/data/maestro_synth/labels"
              mkdir -p "/mnt/storage/data/maestro_synth/logits"
              
              # Create symlinks from project directory to actual data location if needed
              mkdir -p "$PROJ_ROOT/data/synth"
              mkdir -p "$PROJ_ROOT/data/maestro_synth"
              
              # Create symlinks for FMA dataset
              if [ -L "$PROJ_ROOT/data/synth/spectrograms" ]; then
                echo "Symlink for FMA spectrograms already exists"
              else
                ln -sf "/mnt/storage/data/synth/spectrograms" "$PROJ_ROOT/data/synth/spectrograms"
                echo "Created symlink for FMA spectrograms"
              fi
              
              if [ -L "$PROJ_ROOT/data/synth/labels" ]; then
                echo "Symlink for FMA labels already exists"
              else
                ln -sf "/mnt/storage/data/synth/labels" "$PROJ_ROOT/data/synth/labels"
                echo "Created symlink for FMA labels"
              fi
              
              if [ -L "$PROJ_ROOT/data/synth/logits" ]; then
                echo "Symlink for FMA logits already exists"
              else
                ln -sf "/mnt/storage/data/synth/logits" "$PROJ_ROOT/data/synth/logits"
                echo "Created symlink for FMA logits"
              fi
              
              # Create symlinks for Maestro dataset
              if [ -L "$PROJ_ROOT/data/maestro_synth/spectrograms" ]; then
                echo "Symlink for Maestro spectrograms already exists"
              else
                ln -sf "/mnt/storage/data/maestro_synth/spectrograms" "$PROJ_ROOT/data/maestro_synth/spectrograms"
                echo "Created symlink for Maestro spectrograms"
              fi
              
              if [ -L "$PROJ_ROOT/data/maestro_synth/labels" ]; then
                echo "Symlink for Maestro labels already exists"
              else
                ln -sf "/mnt/storage/data/maestro_synth/labels" "$PROJ_ROOT/data/maestro_synth/labels"
                echo "Created symlink for Maestro labels"
              fi
              
              if [ -L "$PROJ_ROOT/data/maestro_synth/logits" ]; then
                echo "Symlink for Maestro logits already exists"
              else
                ln -sf "/mnt/storage/data/maestro_synth/logits" "$PROJ_ROOT/data/maestro_synth/logits"
                echo "Created symlink for Maestro logits"
              fi
              
              # Check for spectrogram files in both datasets
              echo "---- Checking for FMA spectrogram files ----"
              find "/mnt/storage/data/synth" -name "*.npy" | head -5
              
              echo "---- Checking for Maestro spectrogram files ----"
              find "/mnt/storage/data/maestro_synth" -name "*.npy" | head -5
              
              # Check for label files in both datasets
              echo "---- Checking for FMA label files ----"
              find "/mnt/storage/data/synth" -name "*.lab" | head -5
              
              echo "---- Checking for Maestro label files ----"
              find "/mnt/storage/data/maestro_synth" -name "*.lab" | head -5
              
              # Verify there are some relevant files for at least one dataset
              echo "Checking if there are data files to process..."
              FMA_SPEC_COUNT=$(find "/mnt/storage/data/synth" -type f -name "*.npy" | wc -l)
              FMA_LABEL_COUNT=$(find "/mnt/storage/data/synth" -type f -name "*.lab" | wc -l)
              
              MAESTRO_SPEC_COUNT=$(find "/mnt/storage/data/maestro_synth" -type f -name "*.npy" | wc -l)
              MAESTRO_LABEL_COUNT=$(find "/mnt/storage/data/maestro_synth" -type f -name "*.lab" | wc -l)
              
              echo "Found $FMA_SPEC_COUNT FMA spectrogram files and $FMA_LABEL_COUNT FMA label files"
              echo "Found $MAESTRO_SPEC_COUNT Maestro spectrogram files and $MAESTRO_LABEL_COUNT Maestro label files"
              
              if [ "$FMA_SPEC_COUNT" -eq 0 ] && [ "$MAESTRO_SPEC_COUNT" -eq 0 ] || [ "$FMA_LABEL_COUNT" -eq 0 ] && [ "$MAESTRO_LABEL_COUNT" -eq 0 ]; then
                echo "WARNING: No data files found in either dataset. Training may fail."
              else
                echo "Data files found. Proceeding with training."
              fi
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      containers:
        - name: student-trainer
          image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime
          workingDir: /mnt/storage/ChordMini
          imagePullPolicy: IfNotPresent
          env:
            # These environment variables override defaults in student_config.yaml
            - name: PYTHONPATH
              value: /mnt/storage/ChordMini
            # Model scale - overrides student_config.yaml
            - name: MODEL_SCALE
              value: "1.0"  # Default scale: 1.0 = base model, 0.5 = half-scale, 2.0 = double
            # Learning rate scheduling options - overrides student_config.yaml settings
            - name: LR_SCHEDULE
              value: "validation"  # Changed from empty string to 'validation'
            # Additional learning rate parameters - new additions
            - name: LEARNING_RATE
              value: "0.001"  # Base learning rate
            - name: MIN_LEARNING_RATE
              value: "0.00005"  # Minimum learning rate for schedulers
            # Knowledge distillation settings
            - name: USE_KD_LOSS
              value: "true"  # Set to "true" to enable KD loss
            - name: KD_ALPHA
              value: "0.3"  # Weight for KD loss (0-1)
            - name: TEMPERATURE
              value: "2.0"  # Temperature for softening distributions
            # Focal loss settings
            - name: USE_FOCAL_LOSS
              value: "true"  # Set to "true" to enable focal loss
            - name: FOCAL_GAMMA
              value: "2.0"   # Focusing parameter for focal loss
            - name: FOCAL_ALPHA
              value: "0.25"  # Class weight parameter for focal loss
            # Warmup settings
            - name: USE_WARMUP
              value: "true"  # Set to "true" to enable learning rate warm-up
            - name: WARMUP_START_LR
              value: "0.0001"  # Starting learning rate for warm-up
            - name: WARMUP_END_LR
              value: "0.001"  # Final learning rate after warm-up
            - name: WARMUP_EPOCHS
              value: "10"  # Number of epochs for warm-up
            # Dropout setting
            - name: DROPOUT
              value: "0.3"  # Set dropout probability (0-1)
            # Single data root for all data types
            - name: DATA_ROOT
              value: "/mnt/storage/data"
            # GPU memory utilization optimization
            - name: GPU_MEMORY_FRACTION
              value: "0.95"  # Use 95% of available GPU memory
            - name: CUDA_VISIBLE_DEVICES
              value: "0"
            # Dataset GPU optimization flags
            - name: BATCH_GPU_CACHE
              value: "true"  # Enable GPU batch caching
            - name: PREFETCH_FACTOR
              value: "6"     # Prefetch 6 batches for better throughput
            # Add dataset type environment variable
            - name: DATASET_TYPE
              value: "maestro"  # Use "maestro" for Maestro dataset or "fma" for FMA dataset
            # Add checkpoint loading control option
            - name: LOAD_CHECKPOINT
              value: "auto"  # Options: "auto" (load if exists), "never" (always start fresh), "required" (must exist)
          command:
            - sh
            - -c
            - |
              set -ex
              echo "=== Student model training container started ==="
              
              # Install necessary system packages
              apt-get update && apt-get install -y libsndfile1 || { echo "ERROR: Failed to install system dependencies"; exit 1; }
              
              # Create a minimal requirements.txt if missing
              if [ ! -f requirements.txt ]; then
                echo "Creating minimal requirements.txt"
                echo "numpy==1.22.0" > requirements.txt
                echo "librosa>=0.8.0" >> requirements.txt
                echo "torch>=1.9.0" >> requirements.txt
                echo "tqdm>=4.62.0" >> requirements.txt
                echo "scikit-learn>=1.0.0" >> requirements.txt
                echo "pyyaml>=6.0.0" >> requirements.txt  # Added for config loading
              fi
              
              pip install --no-cache-dir -r requirements.txt || { echo "WARNING: Some requirements may have failed"; }
              pip install --no-cache-dir matplotlib tqdm pyyaml librosa scikit-learn || { echo "ERROR: Failed to install additional Python packages"; exit 1; }
              
              # Verify the data paths from the config
              echo "Checking data paths from config..."
              grep -A 10 "paths:" ./config/student_config.yaml
              
              # Double check that the symlinks are still valid
              if [ ! -L "./data/synth/spectrograms" ]; then
                echo "Recreating symlink for spectrograms"
                mkdir -p ./data/synth
                ln -sf "/mnt/storage/data/synth/spectrograms" "./data/synth/spectrograms"
              fi
              
              if [ ! -L "./data/synth/labels" ]; then
                echo "Recreating symlink for labels"
                mkdir -p ./data/synth
                ln -sf "/mnt/storage/data/synth/labels" "./data/synth/labels"
              fi
              
              # Verify that spectrograms and labels exist in the expected location
              SPEC_COUNT=$(find ./data/synth/spectrograms -type f -name "*.npy" | wc -l)
              LABEL_COUNT=$(find ./data/synth/labels -type f -name "*.lab" | wc -l)
              echo "Found $SPEC_COUNT spectrogram files and $LABEL_COUNT label files"
              
              # Before starting training, verify data exists for selected dataset
              if [ "${DATASET_TYPE}" = "maestro" ]; then
                SPEC_COUNT=$(find ./data/maestro_synth/spectrograms -type f -name "*.npy" | wc -l)
                LABEL_COUNT=$(find ./data/maestro_synth/labels -type f -name "*.lab" | wc -l)
                echo "Using Maestro dataset: Found $SPEC_COUNT spectrogram files and $LABEL_COUNT label files"
                
                # Add dataset type to training command
                DATASET_ARG="--dataset_type maestro"
              else
                SPEC_COUNT=$(find ./data/synth/spectrograms -type f -name "*.npy" | wc -l)
                LABEL_COUNT=$(find ./data/synth/labels -type f -name "*.lab" | wc -l)
                echo "Using FMA dataset: Found $SPEC_COUNT spectrogram files and $LABEL_COUNT label files"
                
                # Add dataset type to training command
                DATASET_ARG="--dataset_type fma"
              fi
              
              # Before starting training, verify data exists
              if [ "$SPEC_COUNT" -eq 0 ] || [ "$LABEL_COUNT" -eq 0 ]; then
                echo "WARNING: No data found in the expected paths"
                echo "Looking for data in alternate locations..."
                
                ALT_SPEC_COUNT=$(find /mnt/storage -type f -name "*.npy" | wc -l)
                ALT_LABEL_COUNT=$(find /mnt/storage -type f -name "*.lab" | wc -l)
                echo "Found $ALT_SPEC_COUNT .npy files and $ALT_LABEL_COUNT .lab files in alternate locations"
                
                # Show where the files were found
                if [ "$ALT_SPEC_COUNT" -gt 0 ]; then
                  echo "Sample spectrogram locations:"
                  find /mnt/storage -type f -name "*.npy" | head -5
                fi
                
                if [ "$ALT_LABEL_COUNT" -gt 0 ]; then
                  echo "Sample label locations:"
                  find /mnt/storage -type f -name "*.lab" | head -5
                fi
              fi
              
              # Get LR schedule option from environment
              LR_SCHEDULE_ARG=""
              if [ "${LR_SCHEDULE}" != "" ] && [ "${LR_SCHEDULE}" != "null" ]; then
                echo "Using ${LR_SCHEDULE} learning rate schedule"
                LR_SCHEDULE_ARG="--lr_schedule ${LR_SCHEDULE}"
              elif [ "${USE_WARMUP}" = "true" ]; then
                echo "Using warm-up learning rate schedule for ${WARMUP_EPOCHS} epochs"
                LR_SCHEDULE_ARG="--use_warmup --warmup_epochs ${WARMUP_EPOCHS}"
              else
                echo "Using default validation-based learning rate adjustment"
              fi

              # Get learning rate arguments
              LR_ARGS=""
              if [ "${LEARNING_RATE}" != "" ]; then
                echo "Using base learning rate: ${LEARNING_RATE}"
                LR_ARGS="${LR_ARGS} --learning_rate ${LEARNING_RATE}"
              fi
              
              if [ "${MIN_LEARNING_RATE}" != "" ]; then
                echo "Using minimum learning rate: ${MIN_LEARNING_RATE}"
                LR_ARGS="${LR_ARGS} --min_learning_rate ${MIN_LEARNING_RATE}"
              fi
              
              if [ "${WARMUP_START_LR}" != "" ]; then
                echo "Using warmup start learning rate: ${WARMUP_START_LR}"
                LR_ARGS="${LR_ARGS} --warmup_start_lr ${WARMUP_START_LR}"
              fi
              
              if [ "${WARMUP_END_LR}" != "" ]; then
                echo "Using warmup end learning rate: ${WARMUP_END_LR}"
                LR_ARGS="${LR_ARGS} --warmup_end_lr ${WARMUP_END_LR}"
              fi
              
              # Get focal loss arguments
              FOCAL_LOSS_ARG=""
              if [ "${USE_FOCAL_LOSS}" = "true" ]; then
                echo "Using focal loss with gamma=${FOCAL_GAMMA}"
                FOCAL_LOSS_ARG="--use_focal_loss --focal_gamma ${FOCAL_GAMMA}"
                # Add focal_alpha if specified
                if [ "${FOCAL_ALPHA}" != "" ]; then
                  FOCAL_LOSS_ARG="${FOCAL_LOSS_ARG} --focal_alpha ${FOCAL_ALPHA}"
                  echo "Using focal loss alpha=${FOCAL_ALPHA}"
                fi
              else
                echo "Using standard cross-entropy loss (focal loss disabled)"
              fi
              
              # Get knowledge distillation arguments
              KD_LOSS_ARG=""
              if [ "${USE_KD_LOSS}" = "true" ]; then
                echo "Using knowledge distillation with alpha=${KD_ALPHA} and temperature=${TEMPERATURE}"
                KD_LOSS_ARG="--use_kd_loss --kd_alpha ${KD_ALPHA} --temperature ${TEMPERATURE} --logits_dir ${DATA_ROOT}/logits/synth"
                echo "Note: Teacher logits must be included in the batch data"
              fi
              
              # Get model scale from environment
              MODEL_SCALE_ARG=""
              if [ "${MODEL_SCALE}" != "" ] && [ "${MODEL_SCALE}" != "1.0" ]; then
                echo "Using model scale factor: ${MODEL_SCALE}"
                MODEL_SCALE_ARG="--model_scale ${MODEL_SCALE}"
              else
                echo "Using default model scale (1.0)"
              fi
              
              # Handle caching options for memory optimization
              CACHE_ARGS=""
              if [ "${METADATA_CACHE}" = "true" ]; then
                echo "Using metadata-only caching to reduce memory usage"
                CACHE_ARGS="--metadata_cache"
              else
                echo "Dataset caching disabled to reduce memory usage"
                CACHE_ARGS="--disable_cache"
              fi
              
              # Add cache fraction argument
              if [ "${CACHE_FRACTION}" != "" ] && [ "${CACHE_FRACTION}" != "1.0" ]; then
                echo "Using partial dataset caching: ${CACHE_FRACTION} of samples"
                CACHE_ARGS="${CACHE_ARGS} --cache_fraction ${CACHE_FRACTION}"
              fi
              
              # Handle KD paths when KD is enabled with more flexible configuration
              if [ "${USE_KD_LOSS}" = "true" ]; then
                echo "Knowledge distillation enabled - using teacher logits"
                
                # Check if KD_DATA_ROOT exists
                if [ -d "${DATA_ROOT}/logits/synth" ]; then
                  echo "Using KD data from: ${DATA_ROOT}/logits/synth"
                  
                  # Define paths based on directory structure
                  if [ -d "${DATA_ROOT}/logits/synth/spectrograms" ] && [ -d "${DATA_ROOT}/logits/synth/labels" ] && [ -d "${DATA_ROOT}/logits/synth/logits" ]; then
                    # Standard structure with separate directories
                    # Check if logits directory has more data and use it if it does
                    MAIN_SPEC_COUNT=$(find "${DATA_ROOT}/synth/spectrograms" -type f -name "*.npy" 2>/dev/null | wc -l)
                    LOGITS_SPEC_COUNT=$(find "${DATA_ROOT}/logits/synth/spectrograms" -type f -name "*.npy" 2>/dev/null | wc -l)
                    
                    if [ ${MAIN_SPEC_COUNT:-0} -lt ${LOGITS_SPEC_COUNT:-0} ]; then
                      echo "Using spectrograms and labels from logits directory (more files available)"
                      KD_ARGS="--spec_dir ${DATA_ROOT}/logits/synth/spectrograms --label_dir ${DATA_ROOT}/logits/synth/labels --logits_dir ${DATA_ROOT}/logits/synth/logits"
                    else
                      echo "Using spectrograms and labels from main directory with logits from logits directory"
                      KD_ARGS="--logits_dir ${DATA_ROOT}/logits/synth/logits"
                    fi
                    echo "Using standard KD directory structure"
                  elif [ -d "${DATA_ROOT}/logits/synth" ]; then
                    # All files might be in the root directory
                    KD_ARGS="--spec_dir ${DATA_ROOT}/logits/synth --label_dir ${DATA_ROOT}/logits/synth --logits_dir ${DATA_ROOT}/logits/synth"
                    echo "Using flat KD directory structure"
                  else
                    echo "WARNING: KD_DATA_ROOT does not exist: ${DATA_ROOT}/logits/synth"
                    echo "Attempting to use default paths"
                    KD_ARGS="--logits_dir /mnt/storage/data/logits/synth/logits"
                    
                    # Create parent directories to avoid errors later
                    mkdir -p "/mnt/storage/data/logits/synth/logits"
                  fi
                  
                  # Log the actual paths being used
                  echo "KD arguments: ${KD_ARGS}"
                else
                  KD_ARGS=""
                fi
              else
                KD_ARGS=""
              fi
              
              # GPU acceleration options
              GPU_ARGS=""
              if [ "${GPU_MEMORY_FRACTION}" != "" ]; then
                GPU_ARGS="--gpu_memory_fraction ${GPU_MEMORY_FRACTION}"
              fi
              
              if [ "${BATCH_GPU_CACHE}" = "true" ]; then
                GPU_ARGS="${GPU_ARGS} --batch_gpu_cache"
              fi
              
              if [ "${PREFETCH_FACTOR}" != "" ]; then
                GPU_ARGS="${GPU_ARGS} --prefetch_factor ${PREFETCH_FACTOR}"
              fi
              
              # Get dropout from environment
              DROPOUT_ARG=""
              if [ "${DROPOUT}" != "" ]; then
                echo "Using dropout value: ${DROPOUT}"
                DROPOUT_ARG="--dropout ${DROPOUT}"
              else
                echo "Using default dropout value from config"
              fi
              
              # Run the student training script with all options
              echo "Starting student training with GPU optimization and all options..."
              python train_student.py $DATASET_ARG --config ./config/student_config.yaml --save_dir ${CHECKPOINT_DIR} \
                ${LR_SCHEDULE_ARG} ${FOCAL_LOSS_ARG} ${KD_LOSS_ARG} ${MODEL_SCALE_ARG} ${CACHE_ARGS} ${KD_ARGS}\
                ${GPU_ARGS} ${BATCH_OVERRIDE} ${DROPOUT_ARG} ${LR_ARGS} ${CHECKPOINT_ARG}
              
              echo "=== Student model training complete ==="
              
              # Clear GPU cache to release memory before exiting
              python -c "import torch; torch.cuda.empty_cache() if torch.cuda.is_available() else None"
          resources:
            requests:
              cpu: "3"             
              memory: "18Gi"    
              nvidia.com/gpu: "1"
            limits:
              cpu: "6"             
              memory: "32Gi" 
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      volumes:
        - name: storage
          persistentVolumeClaim:
            claimName: temporary-storage-1