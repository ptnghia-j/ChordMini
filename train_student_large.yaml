apiVersion: batch/v1
kind: Job
metadata:
  name: chord-mini-large
  namespace: csuf-titans
  labels:
    app: chord-student-training
spec:
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: chord-student-training
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - NVIDIA-A10
              - key: kubernetes.io/hostname      # new host pin
                operator: In
                values:
                - gpu-13.nrp.mghpcc.org
      restartPolicy: Never
      tolerations:
        - key: "node.kubernetes.io/not-ready"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
        - key: "node.kubernetes.io/unreachable"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
      initContainers:
        - name: clone-repo
          image: alpine:3.16
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              set -ex
              echo "Installing git..."
              apk update && apk add --no-cache git
              echo "Checking for existing project repository..."
              if [ ! -d "/mnt/storage/ChordMini" ]; then
                echo "Cloning repository..."
                git clone https://gitlab.com/ptnghia-j/ChordMini.git /mnt/storage/ChordMini || {
                  echo "ERROR: Failed to clone repository"
                  exit 1
                }
              else
                echo "Repository already exists."
              fi
              exit 0
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
        - name: data-check
          image: alpine:3.16
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              set -ex
              echo "Checking data directory structure..."

              # Locate the spectrogram and label directories
              PROJ_ROOT="/mnt/storage/ChordMini"
              echo "Project root: $PROJ_ROOT"

              # Create consolidated data directories if they don't exist
              # FMA dataset directories (using logits structure for all)
              mkdir -p "/mnt/storage/data/logits/synth/spectrograms"
              mkdir -p "/mnt/storage/data/logits/synth/labels"
              mkdir -p "/mnt/storage/data/logits/synth/logits"

              # Maestro dataset directories (using logits structure for all)
              mkdir -p "/mnt/storage/data/logits/maestro_synth/spectrograms"
              mkdir -p "/mnt/storage/data/logits/maestro_synth/labels"
              mkdir -p "/mnt/storage/data/logits/maestro_synth/logits"

              # DALI dataset directories (directly under data_root)
              mkdir -p "/mnt/storage/data/dali_synth/spectrograms"
              mkdir -p "/mnt/storage/data/dali_synth/labels"
              mkdir -p "/mnt/storage/data/dali_synth/logits"

              # Create symlinks from project directory to actual data location
              mkdir -p "$PROJ_ROOT/data/synth"
              mkdir -p "$PROJ_ROOT/data/maestro_synth"
              mkdir -p "$PROJ_ROOT/data/dali_synth"

              # Create unified symlinks for FMA dataset
              if [ -L "$PROJ_ROOT/data/synth/spectrograms" ]; then
                echo "Symlink for FMA spectrograms already exists"
              else
                ln -sf "/mnt/storage/data/logits/synth/spectrograms" "$PROJ_ROOT/data/synth/spectrograms"
                echo "Created symlink for FMA spectrograms"
              fi

              if [ -L "$PROJ_ROOT/data/synth/labels" ]; then
                echo "Symlink for FMA labels already exists"
              else
                ln -sf "/mnt/storage/data/logits/synth/labels" "$PROJ_ROOT/data/synth/labels"
                echo "Created symlink for FMA labels"
              fi

              if [ -L "$PROJ_ROOT/data/synth/logits" ]; then
                echo "Symlink for FMA logits already exists"
              else
                ln -sf "/mnt/storage/data/logits/synth/logits" "$PROJ_ROOT/data/synth/logits"
                echo "Created symlink for FMA logits"
              fi

              # Create unified symlinks for Maestro dataset
              if [ -L "$PROJ_ROOT/data/maestro_synth/spectrograms" ]; then
                echo "Symlink for Maestro spectrograms already exists"
              else
                ln -sf "/mnt/storage/data/logits/maestro_synth/spectrograms" "$PROJ_ROOT/data/maestro_synth/spectrograms"
                echo "Created symlink for Maestro spectrograms"
              fi

              if [ -L "$PROJ_ROOT/data/maestro_synth/labels" ]; then
                echo "Symlink for Maestro labels already exists"
              else
                ln -sf "/mnt/storage/data/logits/maestro_synth/labels" "$PROJ_ROOT/data/maestro_synth/labels"
                echo "Created symlink for Maestro labels"
              fi

              if [ -L "$PROJ_ROOT/data/maestro_synth/logits" ]; then
                echo "Symlink for Maestro logits already exists"
              else
                ln -sf "/mnt/storage/data/logits/maestro_synth/logits" "$PROJ_ROOT/data/maestro_synth/logits"
                echo "Created symlink for Maestro logits"
              fi

              # Create unified symlinks for DALI dataset
              if [ -L "$PROJ_ROOT/data/dali_synth/spectrograms" ]; then
                echo "Symlink for DALI spectrograms already exists"
              else
                ln -sf "/mnt/storage/data/dali_synth/spectrograms" "$PROJ_ROOT/data/dali_synth/spectrograms"
                echo "Created symlink for DALI spectrograms"
              fi

              if [ -L "$PROJ_ROOT/data/dali_synth/labels" ]; then
                echo "Symlink for DALI labels already exists"
              else
                ln -sf "/mnt/storage/data/dali_synth/labels" "$PROJ_ROOT/data/dali_synth/labels"
                echo "Created symlink for DALI labels"
              fi

              if [ -L "$PROJ_ROOT/data/dali_synth/logits" ]; then
                echo "Symlink for DALI logits already exists"
              else
                ln -sf "/mnt/storage/data/dali_synth/logits" "$PROJ_ROOT/data/dali_synth/logits"
                echo "Created symlink for DALI logits"
              fi

              # Check for spectrogram files in all datasets
              echo "---- Checking for FMA spectrogram files ----"
              find "/mnt/storage/data/logits/synth/spectrograms" -name "*.npy" | head -5

              echo "---- Checking for Maestro spectrogram files ----"
              find "/mnt/storage/data/logits/maestro_synth/spectrograms" -name "*.npy" | head -5

              echo "---- Checking for DALI spectrogram files ----"
              find "/mnt/storage/data/dali_synth/spectrograms" -name "*.npy" | head -5

              # Check for label files in all datasets
              echo "---- Checking for FMA label files ----"
              find "/mnt/storage/data/logits/synth/labels" -name "*.lab" | head -5

              echo "---- Checking for Maestro label files ----"
              find "/mnt/storage/data/logits/maestro_synth/labels" -name "*.lab" | head -5

              echo "---- Checking for DALI label files ----"
              find "/mnt/storage/data/dali_synth/labels" -name "*.lab" | head -5

              # Verify there are some relevant files for at least one dataset
              echo "Checking if there are data files to process..."
              FMA_SPEC_COUNT=$(find "/mnt/storage/data/logits/synth/spectrograms" -type f -name "*.npy" | wc -l)
              FMA_LABEL_COUNT=$(find "/mnt/storage/data/logits/synth/labels" -type f -name "*.lab" | wc -l)

              MAESTRO_SPEC_COUNT=$(find "/mnt/storage/data/logits/maestro_synth/spectrograms" -type f -name "*.npy" | wc -l)
              MAESTRO_LABEL_COUNT=$(find "/mnt/storage/data/logits/maestro_synth/labels" -type f -name "*.lab" | wc -l)

              DALI_SPEC_COUNT=$(find "/mnt/storage/data/dali_synth/spectrograms" -type f -name "*.npy" | wc -l)
              DALI_LABEL_COUNT=$(find "/mnt/storage/data/dali_synth/labels" -type f -name "*.lab" | wc -l)

              echo "Found $FMA_SPEC_COUNT FMA spectrogram files and $FMA_LABEL_COUNT FMA label files"
              echo "Found $MAESTRO_SPEC_COUNT Maestro spectrogram files and $MAESTRO_LABEL_COUNT Maestro label files"
              echo "Found $DALI_SPEC_COUNT DALI spectrogram files and $DALI_LABEL_COUNT DALI label files"

              if [ "$FMA_SPEC_COUNT" -eq 0 ] && [ "$MAESTRO_SPEC_COUNT" -eq 0 ] && [ "$DALI_SPEC_COUNT" -eq 0 ] || [ "$FMA_LABEL_COUNT" -eq 0 ] && [ "$MAESTRO_LABEL_COUNT" -eq 0 ] && [ "$DALI_LABEL_COUNT" -eq 0 ]; then
                echo "WARNING: No data files found in any dataset. Training may fail."
              else
                echo "Data files found. Proceeding with training."
              fi
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      containers:
        - name: student-trainer
          image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime
          workingDir: /mnt/storage/ChordMini
          imagePullPolicy: IfNotPresent
          env:
            # These environment variables override defaults in student_config.yaml
            - name: PYTHONPATH
              value: /mnt/storage/ChordMini
            # Model scale - overrides student_config.yaml
            - name: MODEL_SCALE
              value: "1.0"  # Default scale: 1.0 = base model, 0.5 = half-scale, 2.0 = double
            # Learning rate scheduling options - overrides student_config.yaml settings
            - name: LR_SCHEDULE
              value: "validation"  # Changed from empty string to 'validation'
            # Additional learning rate parameters - new additions
            - name: LEARNING_RATE
              value: "0.0001"  # Base learning rate
            - name: MIN_LEARNING_RATE
              value: "0.000001"  # Minimum learning rate for schedulers
            # Knowledge distillation settings
            - name: USE_KD_LOSS
              value: "false"  # Set to "true" to enable KD loss
            - name: KD_ALPHA
              value: "0.2"  # Weight for KD loss (0-1)
            - name: TEMPERATURE
              value: "2.0"  # Temperature for softening distributions
            # Focal loss settings
            - name: USE_FOCAL_LOSS
              value: "true"  # Set to "true" to enable focal loss
            - name: FOCAL_GAMMA
              value: "2.0"   # Focusing parameter for focal loss
            - name: FOCAL_ALPHA
              value: "0.35"  # Class weight parameter for focal loss
            # Warmup settings
            - name: USE_WARMUP
              value: "true"  # Set to "true" to enable learning rate warm-up
            - name: WARMUP_START_LR
              value: "0.0001"  # Starting learning rate for warm-up
            - name: WARMUP_END_LR
              value: "0.0003"  # Final learning rate after warm-up
            - name: WARMUP_EPOCHS
              value: "7"  # Number of epochs for warm-up
            # Dropout setting
            - name: DROPOUT
              value: "0.2"  # Set dropout probability (0-1)
            # Single data root for all data types
            - name: DATA_ROOT
              value: "/mnt/storage/data"
            # Add teacher checkpoint path for normalization
            - name: TEACHER_CHECKPOINT
              value: "/mnt/storage/BTC/test/btc_model_large_voca.pt" # Path to the teacher model for norm stats
            # GPU memory utilization optimization
            - name: GPU_MEMORY_FRACTION
              value: "0.95"  # Use 95% of available GPU memory
            # Enable distributed training
            - name: DISTRIBUTED
              value: "true"
            - name: DISTRIBUTED_BACKEND
              value: "nccl"
            # Use all available GPUs
            - name: CUDA_VISIBLE_DEVICES
              value: "0,1"
            # Dataset GPU optimization flags
            - name: BATCH_GPU_CACHE
              value: "true"  # Enable GPU batch caching
            - name: PREFETCH_FACTOR
              value: "6"     # (no effect worker=0) Prefetch 6 batches for better throughput
            # Add dataset type environment variable with combined option
            - name: DATASET_TYPE
              value: "dali_synth"  # Options: "fma", "maestro", "dali_synth", "combined", "fma+maestro", "fma+dali_synth", or "maestro+dali_synth"
            # Add checkpoint loading control option
            - name: LOAD_CHECKPOINT
              value: "never"  # Options: "auto" (load if exists), "never" (always start fresh), "required" (must exist)
            - name: SUB_DIR
              value: "student-md-large"  # This will be used to append to the save directory for checkpoints, can be empty or set to a specific subdirectory if needed
            # Add reset epoch control option
            - name: RESET_EPOCH
              value: "true"  # Whether to reset epoch counter when loading a checkpoint (true/false)
            # Add reset scheduler control option
            - name: RESET_SCHEDULER
              value: "true"  # Whether to reset LR scheduler when resetting epochs (true/false)
            # Dataset initialization controls
            - name: LAZY_INIT
              value: "false"  # Controls dataset lazy initialization (true/false)
            - name: SMALL_DATASET
              value: "0.01"  # Set percentage of dataset to use (0.1 = 10%, null = full dataset)
            - name: METADATA_CACHE
              value: "false"  # Use metadata-only caching to reduce memory usage
          command:
            - sh
            - -c
            - |
              # Set environment variables for distributed training
              export MASTER_ADDR="127.0.0.1"
              export MASTER_PORT="29500"
              export WORLD_SIZE=2
              export RANK=0
              export LOCAL_RANK=0
              set -ex
              echo "=== Student model training container started ==="

              # Install necessary system packages
              apt-get update && apt-get install -y libsndfile1 || { echo "ERROR: Failed to install system dependencies"; exit 1; }

              # Create a minimal requirements.txt if missing
              if [ ! -f requirements.txt ]; then
                echo "Creating minimal requirements.txt"
                echo "numpy==1.22.0" > requirements.txt
                echo "librosa>=0.8.0" >> requirements.txt
                echo "torch>=1.9.0" >> requirements.txt
                echo "tqdm>=4.62.0" >> requirements.txt
                echo "scikit-learn>=1.0.0" >> requirements.txt
                echo "pyyaml>=6.0.0" >> requirements.txt  # Added for config loading
              fi

              pip install --no-cache-dir -r requirements.txt || { echo "WARNING: Some requirements may have failed"; }
              pip install --no-cache-dir matplotlib tqdm pyyaml librosa scikit-learn || { echo "ERROR: Failed to install additional Python packages"; exit 1; }

              # Verify the data paths from the config
              echo "Checking data paths from config..."
              grep -A 10 "paths:" ./config/student_config.yaml

              # Double check that the symlinks are still valid
              if [ ! -L "./data/synth/spectrograms" ]; then
                echo "Recreating symlink for spectrograms"
                mkdir -p ./data/synth
                ln -sf "/mnt/storage/data/logits/synth/spectrograms" "./data/synth/spectrograms"
              fi

              if [ ! -L "./data/synth/labels" ]; then
                echo "Recreating symlink for labels"
                mkdir -p ./data/synth
                ln -sf "/mnt/storage/data/logits/synth/labels" "./data/synth/labels"
              fi

              # Verify that spectrograms and labels exist in the expected location
              SPEC_COUNT=$(find ./data/synth/spectrograms -type f -name "*.npy" | wc -l)
              LABEL_COUNT=$(find ./data/synth/labels -type f -name "*.lab" | wc -l)
              echo "Found $SPEC_COUNT spectrogram files and $LABEL_COUNT label files"

              # Before starting training, verify data exists for selected dataset
              if [ "${DATASET_TYPE}" = "maestro" ]; then
                SPEC_COUNT=$(find ./data/logits/maestro_synth/spectrograms -type f -name "*.npy" | wc -l)
                LABEL_COUNT=$(find ./data/logits/maestro_synth/labels -type f -name "*.lab" | wc -l)
                echo "Using Maestro dataset: Found $SPEC_COUNT spectrogram files and $LABEL_COUNT label files"

                # Add dataset type to training command
                DATASET_ARG="--dataset_type maestro"
              elif [ "${DATASET_TYPE}" = "dali_synth" ]; then
                SPEC_COUNT=$(find ./data/dali_synth/spectrograms -type f -name "*.npy" | wc -l)
                LABEL_COUNT=$(find ./data/dali_synth/labels -type f -name "*.lab" | wc -l)
                echo "Using DALI dataset: Found $SPEC_COUNT spectrogram files and $LABEL_COUNT label files"

                # Add dataset type to training command
                DATASET_ARG="--dataset_type dali_synth"
              elif [ "${DATASET_TYPE}" = "combined" ] || [ "${DATASET_TYPE}" = "fma+maestro" ] || [ "${DATASET_TYPE}" = "fma+dali_synth" ] || [ "${DATASET_TYPE}" = "maestro+dali_synth" ]; then
                # Count files in all datasets
                FMA_SPEC_COUNT=$(find ./data/logits/synth/spectrograms -type f -name "*.npy" | wc -l)
                FMA_LABEL_COUNT=$(find ./data/logits/synth/labels -type f -name "*.lab" | wc -l)
                MAESTRO_SPEC_COUNT=$(find ./data/maestro_synth/spectrograms -type f -name "*.npy" | wc -l)
                MAESTRO_LABEL_COUNT=$(find ./data/maestro_synth/labels -type f -name "*.lab" | wc -l)
                DALI_SPEC_COUNT=$(find ./data/dali_synth/spectrograms -type f -name "*.npy" | wc -l)
                DALI_LABEL_COUNT=$(find ./data/dali_synth/labels -type f -name "*.lab" | wc -l)

                if [ "${DATASET_TYPE}" = "combined" ]; then
                  echo "Using Combined datasets (all three):"
                  echo "- FMA: Found $FMA_SPEC_COUNT spectrogram files and $FMA_LABEL_COUNT label files"
                  echo "- Maestro: Found $MAESTRO_SPEC_COUNT spectrogram files and $MAESTRO_LABEL_COUNT label files"
                  echo "- DALI: Found $DALI_SPEC_COUNT spectrogram files and $DALI_LABEL_COUNT label files"
                elif [ "${DATASET_TYPE}" = "fma+maestro" ]; then
                  echo "Using Combined datasets (FMA + Maestro):"
                  echo "- FMA: Found $FMA_SPEC_COUNT spectrogram files and $FMA_LABEL_COUNT label files"
                  echo "- Maestro: Found $MAESTRO_SPEC_COUNT spectrogram files and $MAESTRO_LABEL_COUNT label files"
                elif [ "${DATASET_TYPE}" = "fma+dali_synth" ]; then
                  echo "Using Combined datasets (FMA + DALI):"
                  echo "- FMA: Found $FMA_SPEC_COUNT spectrogram files and $FMA_LABEL_COUNT label files"
                  echo "- DALI: Found $DALI_SPEC_COUNT spectrogram files and $DALI_LABEL_COUNT label files"
                elif [ "${DATASET_TYPE}" = "maestro+dali_synth" ]; then
                  echo "Using Combined datasets (Maestro + DALI):"
                  echo "- Maestro: Found $MAESTRO_SPEC_COUNT spectrogram files and $MAESTRO_LABEL_COUNT label files"
                  echo "- DALI: Found $DALI_SPEC_COUNT spectrogram files and $DALI_LABEL_COUNT label files"
                fi

                # Set dataset type argument
                DATASET_ARG="--dataset_type ${DATASET_TYPE}"
              else
                SPEC_COUNT=$(find ./data/logits/synth/spectrograms -type f -name "*.npy" | wc -l)
                LABEL_COUNT=$(find ./data/logits/synth/labels -type f -name "*.lab" | wc -l)
                echo "Using FMA dataset: Found $SPEC_COUNT spectrogram files and $LABEL_COUNT label files"

                # Add dataset type to training command
                DATASET_ARG="--dataset_type fma"
              fi

              # Get LR schedule option from environment
              LR_SCHEDULE_ARG=""
              if [ "${LR_SCHEDULE}" != "" ] && [ "${LR_SCHEDULE}" != "null" ]; then
                echo "Using ${LR_SCHEDULE} learning rate schedule"
                LR_SCHEDULE_ARG="--lr_schedule ${LR_SCHEDULE}"
              else
                echo "Using default validation-based learning rate adjustment"
              fi

              # Create separate warmup argument variable that's always set when warmup is enabled
              WARMUP_ARG=""
              if [ "${USE_WARMUP}" = "true" ]; then
                echo "Using warm-up learning rate schedule for ${WARMUP_EPOCHS} epochs"
                WARMUP_ARG="--use_warmup --warmup_epochs ${WARMUP_EPOCHS}"

                # Add the other warmup parameters to ensure they're passed to the script
                if [ "${WARMUP_START_LR}" != "" ]; then
                  WARMUP_ARG="${WARMUP_ARG} --warmup_start_lr ${WARMUP_START_LR}"
                  echo "Using warmup start learning rate: ${WARMUP_START_LR}"
                fi

                if [ "${WARMUP_END_LR}" != "" ]; then
                  WARMUP_ARG="${WARMUP_ARG} --warmup_end_lr ${WARMUP_END_LR}"
                  echo "Using warmup end learning rate: ${WARMUP_END_LR}"
                fi
              fi

              # Get learning rate arguments
              LR_ARGS=""
              if [ "${LEARNING_RATE}" != "" ]; then
                echo "Using base learning rate: ${LEARNING_RATE}"
                LR_ARGS="${LR_ARGS} --learning_rate ${LEARNING_RATE}"
              fi

              if [ "${MIN_LEARNING_RATE}" != "" ]; then
                echo "Using minimum learning rate: ${MIN_LEARNING_RATE}"
                LR_ARGS="${LR_ARGS} --min_learning_rate ${MIN_LEARNING_RATE}"
              fi

              if [ "${WARMUP_START_LR}" != "" ]; then
                echo "Using warmup start learning rate: ${WARMUP_START_LR}"
                LR_ARGS="${LR_ARGS} --warmup_start_lr ${WARMUP_START_LR}"
              fi

              if [ "${WARMUP_END_LR}" != "" ]; then
                echo "Using warmup end learning rate: ${WARMUP_END_LR}"
                LR_ARGS="${LR_ARGS} --warmup_end_lr ${WARMUP_END_LR}"
              fi

              # Get focal loss arguments
              FOCAL_LOSS_ARG=""
              if [ "${USE_FOCAL_LOSS}" = "true" ]; then
                echo "Using focal loss with gamma=${FOCAL_GAMMA}"
                FOCAL_LOSS_ARG="--use_focal_loss --focal_gamma ${FOCAL_GAMMA}"
                if [ "${FOCAL_ALPHA}" != "" ]; then
                  echo "Using alpha=${FOCAL_ALPHA} for focal loss weighting"
                  FOCAL_LOSS_ARG="${FOCAL_LOSS_ARG} --focal_alpha ${FOCAL_ALPHA}"
                fi
              else
                echo "Using standard cross-entropy loss (focal loss disabled)"
              fi

              # Get knowledge distillation arguments
              KD_LOSS_ARG=""
              if [ "${USE_KD_LOSS}" = "true" ]; then
                echo "Using knowledge distillation with alpha=${KD_ALPHA} and temperature=${TEMPERATURE}"
                KD_LOSS_ARG="--use_kd_loss --kd_alpha ${KD_ALPHA} --temperature ${TEMPERATURE} --logits_dir ${DATA_ROOT}/logits/synth/logits"
                echo "Note: Teacher logits must be included in the batch data"
              fi

              # Handle KD paths when KD is enabled with more flexible configuration
              if [ "${USE_KD_LOSS}" = "true" ]; then
                echo "Knowledge distillation enabled - using teacher logits"

                # Define the logits paths based on dataset type
                if [ "${DATASET_TYPE}" = "maestro" ]; then
                  # For Maestro dataset
                  KD_ARGS="--logits_dir ${DATA_ROOT}/logits/maestro_synth/logits"
                  echo "Using Maestro logits from: ${DATA_ROOT}/logits/maestro_synth/logits"
                elif [ "${DATASET_TYPE}" = "dali_synth" ]; then
                  # For DALI dataset
                  KD_ARGS="--logits_dir ${DATA_ROOT}/dali_synth/logits"
                  echo "Using DALI logits from: ${DATA_ROOT}/dali_synth/logits"
                elif [ "${DATASET_TYPE}" = "combined" ] || [ "${DATASET_TYPE}" = "fma+maestro" ] || [ "${DATASET_TYPE}" = "fma+dali_synth" ] || [ "${DATASET_TYPE}" = "maestro+dali_synth" ]; then
                  # For combined datasets, specify the primary FMA logits directory
                  # The train_student.py script will find all other logits automatically
                  KD_ARGS="--logits_dir ${DATA_ROOT}/logits/synth/logits"
                  echo "Using ${DATASET_TYPE} dataset - train_student.py will find logits in standard locations"
                else
                  # Default for FMA dataset
                  KD_ARGS="--logits_dir ${DATA_ROOT}/logits/synth/logits"
                  echo "Using FMA logits from: ${DATA_ROOT}/logits/synth/logits"
                fi
              else
                KD_ARGS=""
              fi

              # Get model scale from environment
              MODEL_SCALE_ARG=""
              if [ "${MODEL_SCALE}" != "" ] && [ "${MODEL_SCALE}" != "1.0" ]; then
                echo "Using model scale factor: ${MODEL_SCALE}"
                MODEL_SCALE_ARG="--model_scale ${MODEL_SCALE}"
              else
                echo "Using default model scale (1.0)"
              fi

              # Handle caching options for memory optimization
              CACHE_ARGS=""
              if [ "${METADATA_CACHE}" = "true" ]; then
                echo "Using metadata-only caching to reduce memory usage"
                CACHE_ARGS="--metadata_cache"
              else
                echo "Dataset caching disabled to reduce memory usage"
                CACHE_ARGS="--disable_cache"
              fi

              # Add cache fraction argument
              if [ "${CACHE_FRACTION}" != "" ] && [ "${CACHE_FRACTION}" != "1.0" ]; then
                echo "Using partial dataset caching: ${CACHE_FRACTION} of samples"
                CACHE_ARGS="${CACHE_ARGS} --cache_fraction ${CACHE_FRACTION}"
              fi

              # GPU acceleration options
              GPU_ARGS=""

              if [ "${BATCH_GPU_CACHE}" = "true" ]; then
                GPU_ARGS="${GPU_ARGS} --batch_gpu_cache"
              fi

              if [ "${PREFETCH_FACTOR}" != "" ]; then
                GPU_ARGS="${GPU_ARGS} --prefetch_factor ${PREFETCH_FACTOR}"
              fi

              # Dataset initialization control arguments
              DATASET_INIT_ARGS=""
              if [ "${LAZY_INIT}" = "true" ]; then
                echo "Using lazy initialization for dataset"
                DATASET_INIT_ARGS="--lazy_init"
              else
                echo "Disabling lazy initialization for dataset (will pre-load all metadata)"
              fi

              if [ "${SMALL_DATASET}" != "" ] && [ "${SMALL_DATASET}" != "null" ]; then
                echo "Using only ${SMALL_DATASET} fraction of dataset (${SMALL_DATASET}%)"
                DATASET_INIT_ARGS="${DATASET_INIT_ARGS} --small_dataset ${SMALL_DATASET}"
              else
                echo "Using full dataset"
              fi

              # Get dropout from environment
              DROPOUT_ARG=""
              if [ "${DROPOUT}" != "" ]; then
                echo "Using dropout value: ${DROPOUT}"
                DROPOUT_ARG="--dropout ${DROPOUT}"
              else
                echo "Using default dropout value from config"
              fi

              # ! NOTE: Define checkpoint
              CHECKPOINTS_DIR="/mnt/storage/checkpoints/btc"

              # ! NOTE: save checkpoint directory defined in env
              if [ "${SUB_DIR}" != "" ]; then
                # Create directory with subdirectory if SUB_DIR is specified
                mkdir -p "${CHECKPOINTS_DIR}/${SUB_DIR}"
                echo "Using checkpoint directory: ${CHECKPOINTS_DIR}/${SUB_DIR}"
              else
                # Create just the base directory
                mkdir -p "${CHECKPOINTS_DIR}"
                echo "Using checkpoint directory: ${CHECKPOINTS_DIR}"
              fi

              # Create local checkpoint directory for backward compatibility
              LOCAL_CHECKPOINTS_DIR="/mnt/storage/ChordMini/checkpoints/btc"
              if [ "${LOCAL_CHECKPOINTS_DIR}" != "${CHECKPOINTS_DIR}" ]; then
                mkdir -p "${LOCAL_CHECKPOINTS_DIR}"
                echo "Also created local checkpoint directory for backward compatibility: ${LOCAL_CHECKPOINTS_DIR}"
              fi

              # Handle checkpoint loading option
              CHECKPOINT_ARG=""
              BEST_CHECKPOINT_PATH="${CHECKPOINTS_DIR}/${SUB_DIR}/student_model_best.pth"
              if [ "${LOAD_CHECKPOINT}" = "auto" ]; then
                if [ -f "${BEST_CHECKPOINT_PATH}" ]; then
                  echo "Found existing checkpoint at ${BEST_CHECKPOINT_PATH}"
                  if [ "${RESET_EPOCH}" = "true" ]; then
                    RESET_ARG="--reset_epoch"
                    if [ "${RESET_SCHEDULER}" = "true" ];then
                      echo "Will load weights but start training from epoch 1 with fresh learning rate schedule"
                      RESET_ARG="${RESET_ARG} --reset_scheduler"
                    else
                      echo "Will load weights but start training from epoch 1 (keeping scheduler state)"
                    fi
                    CHECKPOINT_ARG="--load_checkpoint ${BEST_CHECKPOINT_PATH} ${RESET_ARG}"
                  else
                    echo "Will load weights and continue training from saved epoch"
                    CHECKPOINT_ARG="--load_checkpoint ${BEST_CHECKPOINT_PATH}"
                  fi
                else
                  echo "No existing checkpoint found at ${BEST_CHECKPOINT_PATH}"
                  echo "Starting training from scratch"
                fi
              elif [ "${LOAD_CHECKPOINT}" = "never" ]; then
                echo "Starting fresh training (ignoring any existing checkpoints)"
              elif [ "${LOAD_CHECKPOINT}" = "required" ]; then
                if [ -f "${BEST_CHECKPOINT_PATH}" ]; then
                  if [ "${RESET_EPOCH}" = "true" ]; then
                    RESET_ARG="--reset_epoch"
                    if [ "${RESET_SCHEDULER}" = "true" ]; then
                      echo "Found required checkpoint at ${BEST_CHECKPOINT_PATH}"
                      echo "Will load weights but start training from epoch 1 with fresh learning rate schedule"
                      RESET_ARG="${RESET_ARG} --reset_scheduler"
                    else
                      echo "Found required checkpoint at ${BEST_CHECKPOINT_PATH}"
                      echo "Will load weights but start training from epoch 1 (keeping scheduler state)"
                    fi
                    CHECKPOINT_ARG="--load_checkpoint ${BEST_CHECKPOINT_PATH} ${RESET_ARG}"
                  else
                    echo "Will load weights and continue training from saved epoch"
                    CHECKPOINT_ARG="--load_checkpoint ${BEST_CHECKPOINT_PATH}"
                  fi
                else
                  echo "ERROR: Required checkpoint not found at ${BEST_CHECKPOINT_PATH}"
                  echo "Cannot proceed with training"
                  exit 1
                fi
              fi

              # Simplify data directory paths passed to the python script based on dataset type
              if [ "${DATASET_TYPE}" = "maestro" ]; then
                SPEC_DIR_ARG="--spec_dir ${DATA_ROOT}/logits/maestro_synth/spectrograms"
                LABEL_DIR_ARG="--label_dir ${DATA_ROOT}/logits/maestro_synth/labels"
              elif [ "${DATASET_TYPE}" = "dali_synth" ]; then
                SPEC_DIR_ARG="--spec_dir ${DATA_ROOT}/dali_synth/spectrograms"
                LABEL_DIR_ARG="--label_dir ${DATA_ROOT}/dali_synth/labels"
              elif [ "${DATASET_TYPE}" = "combined" ] || [ "${DATASET_TYPE}" = "fma+maestro" ] || [ "${DATASET_TYPE}" = "fma+dali_synth" ] || [ "${DATASET_TYPE}" = "maestro+dali_synth" ]; then
                # For combined datasets, specify the primary FMA path
                # train_student.py will handle finding all datasets based on the dataset_type
                SPEC_DIR_ARG="--spec_dir ${DATA_ROOT}/logits/synth/spectrograms"
                LABEL_DIR_ARG="--label_dir ${DATA_ROOT}/logits/synth/labels"
              else
                # Default FMA dataset
                SPEC_DIR_ARG="--spec_dir ${DATA_ROOT}/logits/synth/spectrograms"
                LABEL_DIR_ARG="--label_dir ${DATA_ROOT}/logits/synth/labels"
              fi

              # Add teacher checkpoint argument
              TEACHER_CHECKPOINT_ARG=""
              if [ -n "${TEACHER_CHECKPOINT}" ]; then
                echo "Loading normalization from teacher checkpoint: ${TEACHER_CHECKPOINT}"
                TEACHER_CHECKPOINT_ARG="--teacher_checkpoint ${TEACHER_CHECKPOINT}"
              else
                echo "WARNING: No teacher checkpoint specified for normalization. Trainer might calculate its own or use defaults."
              fi

              # Run the student training script with all options and simplified paths
              echo "Starting student training with GPU optimization and all options..."
              # Run with distributed training enabled
              python -m torch.distributed.launch \
                --nproc_per_node=2 \
                --master_addr="127.0.0.1" \
                --master_port="29500" \
                train_student.py \
                --distributed \
                --distributed_backend="nccl" \
                $DATASET_ARG $SPEC_DIR_ARG $LABEL_DIR_ARG \
                --config ./config/student_config.yaml \
                --save_dir ${CHECKPOINTS_DIR}/${SUB_DIR} \
                ${LR_SCHEDULE_ARG} ${WARMUP_ARG} ${FOCAL_LOSS_ARG} ${KD_LOSS_ARG} \
                ${MODEL_SCALE_ARG} ${CACHE_ARGS} ${KD_ARGS} \
                ${GPU_ARGS} ${DROPOUT_ARG} ${LR_ARGS} \
                ${CHECKPOINT_ARG} ${DATASET_INIT_ARGS} ${TEACHER_CHECKPOINT_ARG} # Added teacher checkpoint arg

              echo "=== Student model training complete ==="
          resources:
            requests:
              cpu: "4"
              #memory: "196Gi"
              #memory: "128Gi"
              memory: "64Gi"
              nvidia.com/gpu: "2"
            limits:
              cpu: "18"
              #memory: "296Gi"
              #memory: "160Gi"
              memory: "128Gi"
              nvidia.com/gpu: "2"
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      volumes:
        - name: storage
          persistentVolumeClaim:
            claimName: temporary-storage-1