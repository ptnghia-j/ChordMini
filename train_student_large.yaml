apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
  name: chord-mini-distributed
  namespace: csuf-titans
  labels:
    app: chord-student-training
spec:
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: Never
      template:
        metadata:
          labels:
            app: chord-student-training
          annotations:
            sidecar.istio.io/inject: "false"
        spec:
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: nvidia.com/gpu.product
                    operator: In
                    values:
                    - NVIDIA-GeForce-RTX-3090
          initContainers:
            - name: clone-repo
              image: alpine:3.16
              imagePullPolicy: IfNotPresent
              resources:
                requests:
                  cpu: "100m"
                  memory: "64Mi"
                limits:
                  cpu: "200m"
                  memory: "128Mi"
              command:
                - sh
                - -c
                - |
                  set -ex
                  echo "Installing git..."
                  apk update && apk add --no-cache git
                  echo "Checking for existing project repository..."
                  if [ ! -d "/mnt/storage/ChordMini" ]; then
                    echo "Cloning repository..."
                    git clone https://gitlab.com/ptnghia-j/ChordMini.git /mnt/storage/ChordMini || {
                      echo "ERROR: Failed to clone repository"
                      exit 1
                    }
                  else
                    echo "Repository already exists."
                  fi
                  exit 0
              volumeMounts:
                - name: storage
                  mountPath: /mnt/storage
            - name: data-check
              image: alpine:3.16
              imagePullPolicy: IfNotPresent
              resources:
                requests:
                  cpu: "100m"
                  memory: "64Mi"
                limits:
                  cpu: "200m"
                  memory: "128Mi"
              command:
                - sh
                - -c
                - |
                  set -ex
                  echo "Checking data directory structure..."

                  # Locate the spectrogram and label directories
                  PROJ_ROOT="/mnt/storage/ChordMini"
                  echo "Project root: $PROJ_ROOT"

                  # Create consolidated data directories if they don't exist
                  # FMA dataset directories (using logits structure for all)
                  mkdir -p "/mnt/storage/data/logits/synth/spectrograms"
                  mkdir -p "/mnt/storage/data/logits/synth/labels"
                  mkdir -p "/mnt/storage/data/logits/synth/logits"

                  # Maestro dataset directories (using logits structure for all)
                  mkdir -p "/mnt/storage/data/logits/maestro_synth/spectrograms"
                  mkdir -p "/mnt/storage/data/logits/maestro_synth/labels"
                  mkdir -p "/mnt/storage/data/logits/maestro_synth/logits"

                  # Create symlinks from project directory to actual data location
                  mkdir -p "$PROJ_ROOT/data/synth"
                  mkdir -p "$PROJ_ROOT/data/maestro_synth"

                  # Create unified symlinks for FMA dataset
                  if [ -L "$PROJ_ROOT/data/synth/spectrograms" ]; then
                    echo "Symlink for FMA spectrograms already exists"
                  else
                    ln -sf "/mnt/storage/data/logits/synth/spectrograms" "$PROJ_ROOT/data/synth/spectrograms"
                    echo "Created symlink for FMA spectrograms"
                  fi

                  if [ -L "$PROJ_ROOT/data/synth/labels" ]; then
                    echo "Symlink for FMA labels already exists"
                  else
                    ln -sf "/mnt/storage/data/logits/synth/labels" "$PROJ_ROOT/data/synth/labels"
                    echo "Created symlink for FMA labels"
                  fi

                  if [ -L "$PROJ_ROOT/data/synth/logits" ]; then
                    echo "Symlink for FMA logits already exists"
                  else
                    ln -sf "/mnt/storage/data/logits/synth/logits" "$PROJ_ROOT/data/synth/logits"
                    echo "Created symlink for FMA logits"
                  fi

                  # Create unified symlinks for Maestro dataset
                  if [ -L "$PROJ_ROOT/data/maestro_synth/spectrograms" ]; then
                    echo "Symlink for Maestro spectrograms already exists"
                  else
                    ln -sf "/mnt/storage/data/logits/maestro_synth/spectrograms" "$PROJ_ROOT/data/maestro_synth/spectrograms"
                    echo "Created symlink for Maestro spectrograms"
                  fi

                  if [ -L "$PROJ_ROOT/data/maestro_synth/labels" ]; then
                    echo "Symlink for Maestro labels already exists"
                  else
                    ln -sf "/mnt/storage/data/logits/maestro_synth/labels" "$PROJ_ROOT/data/maestro_synth/labels"
                    echo "Created symlink for Maestro labels"
                  fi

                  if [ -L "$PROJ_ROOT/data/maestro_synth/logits" ]; then
                    echo "Symlink for Maestro logits already exists"
                  else
                    ln -sf "/mnt/storage/data/logits/maestro_synth/logits" "$PROJ_ROOT/data/maestro_synth/logits"
                    echo "Created symlink for Maestro logits"
                  fi

                  # Check for spectrogram files in both datasets
                  echo "---- Checking for FMA spectrogram files ----"
                  find "/mnt/storage/data/logits/synth/spectrograms" -name "*.npy" | head -5

                  echo "---- Checking for Maestro spectrogram files ----"
                  find "/mnt/storage/data/logits/maestro_synth/spectrograms" -name "*.npy" | head -5

                  # Check for label files in both datasets
                  echo "---- Checking for FMA label files ----"
                  find "/mnt/storage/data/logits/synth/labels" -name "*.lab" | head -5

                  echo "---- Checking for Maestro label files ----"
                  find "/mnt/storage/data/logits/maestro_synth/labels" -name "*.lab" | head -5

                  # Verify there are some relevant files for at least one dataset
                  echo "Checking if there are data files to process..."
                  FMA_SPEC_COUNT=$(find "/mnt/storage/data/logits/synth/spectrograms" -type f -name "*.npy" | wc -l)
                  FMA_LABEL_COUNT=$(find "/mnt/storage/data/logits/synth/labels" -type f -name "*.lab" | wc -l)

                  MAESTRO_SPEC_COUNT=$(find "/mnt/storage/data/logits/maestro_synth/spectrograms" -type f -name "*.npy" | wc -l)
                  MAESTRO_LABEL_COUNT=$(find "/mnt/storage/data/logits/maestro_synth/labels" -type f -name "*.lab" | wc -l)

                  echo "Found $FMA_SPEC_COUNT FMA spectrogram files and $FMA_LABEL_COUNT FMA label files"
                  echo "Found $MAESTRO_SPEC_COUNT Maestro spectrogram files and $MAESTRO_LABEL_COUNT Maestro label files"

                  if [ "$FMA_SPEC_COUNT" -eq 0 ] && [ "$MAESTRO_SPEC_COUNT" -eq 0 ] || [ "$FMA_LABEL_COUNT" -eq 0 ] && [ "$MAESTRO_LABEL_COUNT" -eq 0 ]; then
                    echo "WARNING: No data files found in either dataset. Training may fail."
                  else
                    echo "Data files found. Proceeding with training."
                  fi
              volumeMounts:
                - name: storage
                  mountPath: /mnt/storage
          containers:
            - name: pytorch
              image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime # Consider using a newer image if possible
              workingDir: /mnt/storage/ChordMini
              imagePullPolicy: IfNotPresent
              env:
                # Keep necessary environment variables
                - name: PYTHONPATH
                  value: /mnt/storage/ChordMini
                - name: MODEL_SCALE
                  value: "1.0"
                - name: LR_SCHEDULE
                  value: "validation"
                # Remove MASTER_ADDR, MASTER_PORT, WORLD_SIZE, RANK - torchrun handles these
                - name: DISTRIBUTED_BACKEND # Keep backend if needed, though often default is fine
                  value: "nccl"
                # Add DATA_ROOT if used by the script indirectly
                - name: DATA_ROOT
                  value: "/mnt/storage/data"
                # Add other necessary env vars like USE_KD_LOSS, KD_ALPHA etc. if they are read by the script
                # Example:
                # - name: USE_KD_LOSS
                #   value: "true"

              command:
                - sh
                - -c
                - |
                  set -ex
                  echo "=== Master node started ==="

                  # Install dependencies (same as before)
                  apt-get update && apt-get install -y libsndfile1 || { echo "ERROR: Failed to install system dependencies"; exit 1; }
                  if [ ! -f requirements.txt ]; then
                    echo "Creating minimal requirements.txt"
                    echo "numpy==1.22.0" > requirements.txt
                    echo "librosa>=0.8.0" >> requirements.txt
                    echo "torch>=1.9.0" >> requirements.txt
                    echo "tqdm>=4.62.0" >> requirements.txt
                    echo "scikit-learn>=1.0.0" >> requirements.txt
                    echo "pyyaml>=6.0.0" >> requirements.txt  # Added for config loading
                  fi
                  pip install --no-cache-dir -r requirements.txt || { echo "WARNING: Some requirements may have failed"; }
                  pip install --no-cache-dir matplotlib tqdm pyyaml librosa scikit-learn || { echo "ERROR: Failed to install additional Python packages"; exit 1; }

                  echo "Master node ready, starting training via torchrun..."

                  # Execute the script directly. torchrun (managed by PyTorchJob) will handle launching nproc_per_node processes.
                  # Remove --distributed, --local_rank, --rank, --world_size, --dist_url from the python command.
                  # Rely on environment variables set by torchrun.
                  # Ensure DATA_ROOT is correctly used or paths are absolute/relative to workingDir.
                  python train_student.py \
                    --dataset_type combined \
                    --spec_dir ${DATA_ROOT}/logits/synth/spectrograms \
                    --label_dir ${DATA_ROOT}/logits/synth/labels \
                    --config ./config/student_config.yaml \
                    --save_dir /mnt/storage/checkpoints/student/combined-logits-02 \
                    --lr_schedule validation \
                    --use_warmup \
                    --warmup_epochs 10 \
                    --use_focal_loss \
                    --focal_gamma 2.0 \
                    --focal_alpha 0.25 \
                    --use_kd_loss \
                    --kd_alpha 0.2 \
                    --temperature 3.0 \
                    --logits_dir ${DATA_ROOT}/logits/synth/logits \
                    --gpu_memory_fraction 0.95 \
                    --batch_gpu_cache \
                    --prefetch_factor 6 \
                    --learning_rate 0.001 \
                    --min_learning_rate 0.00001 \
                    --warmup_start_lr 0.0004 \
                    --warmup_end_lr 0.001 \
                    --dropout 0.3 \
                    --small_dataset 0.05
                    # Add any other necessary arguments from the original command

              resources:
                requests:
                  cpu: "4" # Increased CPU request for 2 processes
                  memory: "16Gi" # Increased memory request
                  nvidia.com/gpu: "2" # Request 2 GPUs
                limits:
                  cpu: "8" # Increased CPU limit
                  memory: "32Gi" # Increased memory limit
                  nvidia.com/gpu: "2" # Limit to 2 GPUs
              volumeMounts:
                - name: storage
                  mountPath: /mnt/storage
          volumes:
            - name: storage
              persistentVolumeClaim:
                claimName: temporary-storage-1
    Worker:
      replicas: 1 # Enable one worker node for distributed training
      restartPolicy: Never
      template:
        metadata:
          labels:
            app: chord-student-training
          annotations:
            sidecar.istio.io/inject: "false"
        spec:
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: nvidia.com/gpu.product
                    operator: In
                    values:
                    - NVIDIA-GeForce-RTX-3090
          tolerations:
            - key: "node.kubernetes.io/not-ready"
              operator: "Exists"
              effect: "NoExecute"
              tolerationSeconds: 86400
            - key: "node.kubernetes.io/unreachable"
              operator: "Exists"
              effect: "NoExecute"
              tolerationSeconds: 86400
          initContainers:
            - name: clone-repo
              image: alpine:3.16
              imagePullPolicy: IfNotPresent
              resources:
                requests:
                  cpu: "100m"
                  memory: "64Mi"
                limits:
                  cpu: "200m"
                  memory: "128Mi"
              command:
                - sh
                - -c
                - |
                  set -ex
                  echo "Installing git..."
                  apk update && apk add --no-cache git
                  echo "Checking for existing project repository..."
                  if [ ! -d "/mnt/storage/ChordMini" ]; then
                    echo "Cloning repository..."
                    git clone https://gitlab.com/ptnghia-j/ChordMini.git /mnt/storage/ChordMini || {
                      echo "ERROR: Failed to clone repository"
                      exit 1
                    }
                  else
                    echo "Repository already exists."
                  fi
                  exit 0
              volumeMounts:
                - name: storage
                  mountPath: /mnt/storage
            - name: data-check
              image: alpine:3.16
              imagePullPolicy: IfNotPresent
              resources:
                requests:
                  cpu: "100m"
                  memory: "64Mi"
                limits:
                  cpu: "200m"
                  memory: "128Mi"
              command:
                - sh
                - -c
                - |
                  set -ex
                  echo "Checking data directory structure..."

                  # Locate the spectrogram and label directories
                  PROJ_ROOT="/mnt/storage/ChordMini"
                  echo "Project root: $PROJ_ROOT"

                  # Create consolidated data directories if they don't exist
                  # FMA dataset directories (using logits structure for all)
                  mkdir -p "/mnt/storage/data/logits/synth/spectrograms"
                  mkdir -p "/mnt/storage/data/logits/synth/labels"
                  mkdir -p "/mnt/storage/data/logits/synth/logits"

                  # Maestro dataset directories (using logits structure for all)
                  mkdir -p "/mnt/storage/data/logits/maestro_synth/spectrograms"
                  mkdir -p "/mnt/storage/data/logits/maestro_synth/labels"
                  mkdir -p "/mnt/storage/data/logits/maestro_synth/logits"

                  # Create symlinks from project directory to actual data location
                  mkdir -p "$PROJ_ROOT/data/synth"
                  mkdir -p "$PROJ_ROOT/data/maestro_synth"

                  # Create unified symlinks for FMA dataset
                  if [ -L "$PROJ_ROOT/data/synth/spectrograms" ]; then
                    echo "Symlink for FMA spectrograms already exists"
                  else
                    ln -sf "/mnt/storage/data/logits/synth/spectrograms" "$PROJ_ROOT/data/synth/spectrograms"
                    echo "Created symlink for FMA spectrograms"
                  fi

                  if [ -L "$PROJ_ROOT/data/synth/labels" ]; then
                    echo "Symlink for FMA labels already exists"
                  else
                    ln -sf "/mnt/storage/data/logits/synth/labels" "$PROJ_ROOT/data/synth/labels"
                    echo "Created symlink for FMA labels"
                  fi

                  if [ -L "$PROJ_ROOT/data/synth/logits" ]; then
                    echo "Symlink for FMA logits already exists"
                  else
                    ln -sf "/mnt/storage/data/logits/synth/logits" "$PROJ_ROOT/data/synth/logits"
                    echo "Created symlink for FMA logits"
                  fi

                  # Create unified symlinks for Maestro dataset
                  if [ -L "$PROJ_ROOT/data/maestro_synth/spectrograms" ]; then
                    echo "Symlink for Maestro spectrograms already exists"
                  else
                    ln -sf "/mnt/storage/data/logits/maestro_synth/spectrograms" "$PROJ_ROOT/data/maestro_synth/spectrograms"
                    echo "Created symlink for Maestro spectrograms"
                  fi

                  if [ -L "$PROJ_ROOT/data/maestro_synth/labels" ]; then
                    echo "Symlink for Maestro labels already exists"
                  else
                    ln -sf "/mnt/storage/data/logits/maestro_synth/labels" "$PROJ_ROOT/data/maestro_synth/labels"
                    echo "Created symlink for Maestro labels"
                  fi

                  if [ -L "$PROJ_ROOT/data/maestro_synth/logits" ]; then
                    echo "Symlink for Maestro logits already exists"
                  else
                    ln -sf "/mnt/storage/data/logits/maestro_synth/logits" "$PROJ_ROOT/data/maestro_synth/logits"
                    echo "Created symlink for Maestro logits"
                  fi

                  # Check for spectrogram files in both datasets
                  echo "---- Checking for FMA spectrogram files ----"
                  find "/mnt/storage/data/logits/synth/spectrograms" -name "*.npy" | head -5

                  echo "---- Checking for Maestro spectrogram files ----"
                  find "/mnt/storage/data/logits/maestro_synth/spectrograms" -name "*.npy" | head -5

                  # Check for label files in both datasets
                  echo "---- Checking for FMA label files ----"
                  find "/mnt/storage/data/logits/synth/labels" -name "*.lab" | head -5

                  echo "---- Checking for Maestro label files ----"
                  find "/mnt/storage/data/logits/maestro_synth/labels" -name "*.lab" | head -5

                  # Verify there are some relevant files for at least one dataset
                  echo "Checking if there are data files to process..."
                  FMA_SPEC_COUNT=$(find "/mnt/storage/data/logits/synth/spectrograms" -type f -name "*.npy" | wc -l)
                  FMA_LABEL_COUNT=$(find "/mnt/storage/data/logits/synth/labels" -type f -name "*.lab" | wc -l)

                  MAESTRO_SPEC_COUNT=$(find "/mnt/storage/data/logits/maestro_synth/spectrograms" -type f -name "*.npy" | wc -l)
                  MAESTRO_LABEL_COUNT=$(find "/mnt/storage/data/logits/maestro_synth/labels" -type f -name "*.lab" | wc -l)

                  echo "Found $FMA_SPEC_COUNT FMA spectrogram files and $FMA_LABEL_COUNT FMA label files"
                  echo "Found $MAESTRO_SPEC_COUNT Maestro spectrogram files and $MAESTRO_LABEL_COUNT Maestro label files"

                  if [ "$FMA_SPEC_COUNT" -eq 0 ] && [ "$MAESTRO_SPEC_COUNT" -eq 0 ] || [ "$FMA_LABEL_COUNT" -eq 0 ] && [ "$MAESTRO_LABEL_COUNT" -eq 0 ]; then
                    echo "WARNING: No data files found in either dataset. Training may fail."
                  else
                    echo "Data files found. Proceeding with training."
                  fi
              volumeMounts:
                - name: storage
                  mountPath: /mnt/storage
          containers:
            - name: pytorch
              image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime # Consider using a newer image if possible
              workingDir: /mnt/storage/ChordMini
              imagePullPolicy: IfNotPresent

              command:
                - sh
                - -c
                - |
                  set -ex
                  echo "=== Worker node started ==="

                  # Install dependencies (same as before)
                  apt-get update && apt-get install -y libsndfile1 || { echo "ERROR: Failed to install system dependencies"; exit 1; }
                  if [ ! -f requirements.txt ]; then
                    echo "Creating minimal requirements.txt"
                    echo "numpy==1.22.0" > requirements.txt
                    echo "librosa>=0.8.0" >> requirements.txt
                    echo "torch>=1.9.0" >> requirements.txt
                    echo "tqdm>=4.62.0" >> requirements.txt
                    echo "scikit-learn>=1.0.0" >> requirements.txt
                    echo "pyyaml>=6.0.0" >> requirements.txt  # Added for config loading
                  fi
                  pip install --no-cache-dir -r requirements.txt || { echo "WARNING: Some requirements may have failed"; }
                  pip install --no-cache-dir matplotlib tqdm pyyaml librosa scikit-learn || { echo "ERROR: Failed to install additional Python packages"; exit 1; }

                  echo "Worker node ready, starting training via torchrun..."

                  # Execute the script directly. torchrun (managed by PyTorchJob) will handle launching nproc_per_node processes.
                  python train_student.py \
                    --dataset_type combined \
                    --spec_dir ${DATA_ROOT}/logits/synth/spectrograms \
                    --label_dir ${DATA_ROOT}/logits/synth/labels \
                    --config ./config/student_config.yaml \
                    --save_dir /mnt/storage/checkpoints/student/combined-logits-02 \
                    --lr_schedule validation \
                    --use_warmup \
                    --warmup_epochs 10 \
                    --use_focal_loss \
                    --focal_gamma 2.0 \
                    --focal_alpha 0.25 \
                    --use_kd_loss \
                    --kd_alpha 0.2 \
                    --temperature 3.0 \
                    --logits_dir ${DATA_ROOT}/logits/synth/logits \
                    --gpu_memory_fraction 0.95 \
                    --batch_gpu_cache \
                    --prefetch_factor 6 \
                    --learning_rate 0.001 \
                    --min_learning_rate 0.00001 \
                    --warmup_start_lr 0.0004 \
                    --warmup_end_lr 0.001 \
                    --dropout 0.3 \
                    --small_dataset 0.05
              env:
                # Keep necessary environment variables
                - name: PYTHONPATH
                  value: /mnt/storage/ChordMini
                - name: MODEL_SCALE
                  value: "1.0"
                - name: LR_SCHEDULE
                  value: "validation"
                # Remove MASTER_ADDR, MASTER_PORT, WORLD_SIZE, RANK - torchrun handles these
                - name: DISTRIBUTED_BACKEND # Keep backend if needed, though often default is fine
                  value: "nccl"
                # Add DATA_ROOT if used by the script indirectly
                - name: DATA_ROOT
                  value: "/mnt/storage/data"
              resources:
                requests:
                  cpu: "2"
                  memory: "8Gi"
                  nvidia.com/gpu: "1"
                limits:
                  cpu: "4"
                  memory: "16Gi"
                  nvidia.com/gpu: "1"

            - name: sidecar
              image: alpine:3.16
              command:
                - sh
                - -c
                - |
                  echo "=== Sidecar started ==="
                  sleep 3600
              resources:
                limits:
                  cpu: "100m"
                  memory: "32Mi"
                requests:
                  cpu: "50m"
                  memory: "16Mi"
              volumeMounts:
                - name: storage
                  mountPath: /mnt/storage


          volumes:
            - name: storage
              persistentVolumeClaim:
                claimName: temporary-storage-1