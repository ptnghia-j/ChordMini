apiVersion: batch/v1
kind: Job
metadata:
  name: chord-cv-kd-01
  namespace: csuf-titans
  labels:
    app: chord-cv-kd-training
spec:
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: chord-cv-kd-training
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - NVIDIA-A10
      restartPolicy: Never
      tolerations:
        - key: "node.kubernetes.io/not-ready"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
        - key: "node.kubernetes.io/unreachable"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 86400
      initContainers:
        - name: clone-repo
          image: alpine:3.16
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - |
              set -ex
              echo "Installing git..."
              apk update && apk add --no-cache git
              echo "Checking for existing project repository..."
              if [ ! -d "/mnt/storage/ChordMini" ]; then
                echo "Cloning repository..."
                git clone https://gitlab.com/ptnghia-j/ChordMini.git /mnt/storage/ChordMini || {
                  echo "ERROR: Failed to clone repository"
                  exit 1
                }
              else
                echo "Repository already exists."
              fi
              exit 0
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      containers:
        - name: cv-kd-trainer
          image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime
          workingDir: /mnt/storage/ChordMini
          imagePullPolicy: IfNotPresent
          env:
            # These environment variables override defaults in student_config.yaml
            - name: PYTHONPATH
              value: /mnt/storage/ChordMini
            # Model settings
            - name: MODEL_SCALE
              value: "1.0"  # Scale factor for model size (1.0 = standard, 2.0 = 2x larger)
            - name: MODEL_TYPE
              value: "ChordNet"  # Model type: ChordNet or BTC
            - name: BTC_CHECKPOINT
              value: "/mnt/storage/checkpoints/btc/btc_model_best.pth"  # Path to BTC model checkpoint
            # Learning rate scheduling options
            - name: LR_SCHEDULE
              value: "cosine"  # Options: cosine, step, validation
            # Additional learning rate parameters
            - name: LEARNING_RATE
              value: "0.0001"  # Base learning rate
            - name: MIN_LEARNING_RATE
              value: "0.000001"  # Minimum learning rate for schedulers
            # Knowledge distillation settings
            - name: USE_KD_LOSS
              value: "false"  # Set to "true" to enable KD loss
            - name: KD_ALPHA
              value: "0.2"  # Weight for KD loss (0-1)
            - name: TEMPERATURE
              value: "4.0"  # Temperature for softening distributions
            - name: TEACHER_MODEL
              value: "/mnt/storage/checkpoints/btc/btc_model_large_voca.pt"  # Path to teacher model
            # Focal loss settings
            - name: USE_FOCAL_LOSS
              value: "true"  # Set to "true" to enable focal loss
            - name: FOCAL_GAMMA
              value: "2.0"   # Focusing parameter for focal loss
            - name: FOCAL_ALPHA
              value: "0.25"  # Class weight parameter for focal loss
            # Warmup settings
            - name: USE_WARMUP
              value: "true"  # Set to "true" to enable learning rate warm-up
            - name: WARMUP_START_LR
              value: "0.0001"  # Starting learning rate for warm-up
            - name: WARMUP_END_LR
              value: "0.0005"  # Final learning rate after warm-up
            - name: WARMUP_EPOCHS
              value: "10"  # Number of epochs for warm-up
            # Dropout setting
            - name: DROPOUT
              value: "0.3"  # Set dropout probability (0-1)
            # Cross-validation settings
            - name: KFOLD
              value: "0"  # Which fold to use for validation (0-4)
            - name: TOTAL_FOLDS
              value: "5"  # Total number of folds
            # Single data root for all data types
            - name: DATA_ROOT
              value: "/mnt/storage/data"
            # GPU memory utilization optimization
            - name: GPU_MEMORY_FRACTION
              value: "0.95"  # Use 95% of available GPU memory
            - name: CUDA_VISIBLE_DEVICES
              value: "0"
            # Dataset GPU optimization flags
            - name: BATCH_GPU_CACHE
              value: "true"  # Enable GPU batch caching
            - name: PREFETCH_FACTOR
              value: "6"     # ! NO EFFECT if num_workers=0
            # Add checkpoint loading control option
            - name: LOAD_CHECKPOINT
              value: "never"  # Options: "auto" (load if exists), "never" (always start fresh), "required" (must exist)
            - name: SUB_DIR
              value: "cv-kd-01"  # This will be used to append to the save directory for checkpoints
            # Add reset epoch control option
            - name: RESET_EPOCH
              value: "true"  # Whether to reset epoch counter when loading a checkpoint (true/false)
            # Add reset scheduler control option
            - name: RESET_SCHEDULER
              value: "true"  # Whether to reset LR scheduler when resetting epochs (true/false)
            # Dataset initialization controls
            - name: LAZY_INIT
              value: "false"  # Controls dataset lazy initialization (true/false)
            - name: SMALL_DATASET
              value: ""  # Set percentage of dataset to use (0.1 = 10%, null = full dataset)
            - name: METADATA_CACHE
              value: "false"  # Use metadata-only caching to reduce memory usage
          command:
            - sh
            - -c
            - |
              set -ex
              echo "=== Cross-validation with Knowledge Distillation training container started ==="

              # Install necessary system packages
              apt-get update && apt-get install -y libsndfile1 || { echo "ERROR: Failed to install system dependencies"; exit 1; }

              # Create a minimal requirements.txt if missing
              if [ ! -f requirements.txt ]; then
                echo "Creating minimal requirements.txt"
                echo "numpy==1.22.0" > requirements.txt
                echo "librosa>=0.8.0" >> requirements.txt
                echo "torch>=1.9.0" >> requirements.txt
                echo "tqdm>=4.62.0" >> requirements.txt
                echo "scikit-learn>=1.0.0" >> requirements.txt
                echo "pyyaml>=6.0.0" >> requirements.txt  # Added for config loading
              fi

              pip install --no-cache-dir -r requirements.txt || { echo "WARNING: Some requirements may have failed"; }
              pip install --no-cache-dir matplotlib tqdm pyyaml librosa scikit-learn || { echo "ERROR: Failed to install additional Python packages"; exit 1; }

              # Get LR schedule option from environment
              LR_SCHEDULE_ARG=""
              if [ "${LR_SCHEDULE}" != "" ] && [ "${LR_SCHEDULE}" != "null" ]; then
                echo "Using ${LR_SCHEDULE} learning rate schedule"
                LR_SCHEDULE_ARG="--lr_schedule ${LR_SCHEDULE}"
              else
                echo "Using default validation-based learning rate adjustment"
              fi

              # Create separate warmup argument variable that's always set when warmup is enabled
              WARMUP_ARG=""
              if [ "${USE_WARMUP}" = "true" ]; then
                echo "Using warm-up learning rate schedule for ${WARMUP_EPOCHS} epochs"
                WARMUP_ARG="--use_warmup --warmup_epochs ${WARMUP_EPOCHS}"

                # Add the other warmup parameters to ensure they're passed to the script
                if [ "${WARMUP_START_LR}" != "" ]; then
                  WARMUP_ARG="${WARMUP_ARG} --warmup_start_lr ${WARMUP_START_LR}"
                  echo "Using warmup start learning rate: ${WARMUP_START_LR}"
                fi

                if [ "${WARMUP_END_LR}" != "" ]; then
                  WARMUP_ARG="${WARMUP_ARG} --warmup_end_lr ${WARMUP_END_LR}"
                  echo "Using warmup end learning rate: ${WARMUP_END_LR}"
                fi
              fi

              # Get learning rate arguments
              LR_ARGS=""
              if [ "${LEARNING_RATE}" != "" ]; then
                echo "Using base learning rate: ${LEARNING_RATE}"
                LR_ARGS="${LR_ARGS} --learning_rate ${LEARNING_RATE}"
              fi

              if [ "${MIN_LEARNING_RATE}" != "" ]; then
                echo "Using minimum learning rate: ${MIN_LEARNING_RATE}"
                LR_ARGS="${LR_ARGS} --min_learning_rate ${MIN_LEARNING_RATE}"
              fi

              # Get focal loss arguments
              FOCAL_LOSS_ARG=""
              if [ "${USE_FOCAL_LOSS}" = "true" ]; then
                echo "Using focal loss with gamma=${FOCAL_GAMMA}"
                FOCAL_LOSS_ARG="--use_focal_loss --focal_gamma ${FOCAL_GAMMA}"
                # Add focal_alpha if specified
                if [ "${FOCAL_ALPHA}" != "" ]; then
                  FOCAL_LOSS_ARG="${FOCAL_LOSS_ARG} --focal_alpha ${FOCAL_ALPHA}"
                  echo "Using focal loss alpha=${FOCAL_ALPHA}"
                fi
              else
                echo "Using standard cross-entropy loss (focal loss disabled)"
              fi

              # Get knowledge distillation arguments
              KD_LOSS_ARG=""
              if [ "${USE_KD_LOSS}" = "true" ]; then
                echo "Using knowledge distillation with alpha=${KD_ALPHA} and temperature=${TEMPERATURE}"
                KD_LOSS_ARG="--use_kd_loss --kd_alpha ${KD_ALPHA} --temperature ${TEMPERATURE}"
                
                # Add teacher model path if specified
                if [ "${TEACHER_MODEL}" != "" ]; then
                  KD_LOSS_ARG="${KD_LOSS_ARG} --teacher_model ${TEACHER_MODEL}"
                  echo "Using teacher model: ${TEACHER_MODEL}"
                fi
              fi

              # Get model scale from environment
              MODEL_SCALE_ARG=""
              if [ "${MODEL_SCALE}" != "" ] && [ "${MODEL_SCALE}" != "1.0" ]; then
                echo "Using model scale factor: ${MODEL_SCALE}"
                MODEL_SCALE_ARG="--model_scale ${MODEL_SCALE}"
              else
                echo "Using default model scale (1.0)"
              fi
              
              # Get model type arguments
              MODEL_TYPE_ARGS=""
              if [ "${MODEL_TYPE}" != "" ]; then
                echo "Using model type: ${MODEL_TYPE}"
                MODEL_TYPE_ARGS="--model_type ${MODEL_TYPE}"
                
                # Add BTC checkpoint if model type is BTC
                if [ "${MODEL_TYPE}" = "BTC" ] && [ "${BTC_CHECKPOINT}" != "" ]; then
                  MODEL_TYPE_ARGS="${MODEL_TYPE_ARGS} --btc_checkpoint ${BTC_CHECKPOINT}"
                  echo "Using BTC checkpoint: ${BTC_CHECKPOINT}"
                fi
              fi

              # Get cross-validation arguments
              CV_ARGS=""
              if [ "${KFOLD}" != "" ]; then
                echo "Using fold ${KFOLD} for validation"
                CV_ARGS="--kfold ${KFOLD}"
              fi
              
              if [ "${TOTAL_FOLDS}" != "" ]; then
                echo "Using ${TOTAL_FOLDS} total folds"
                CV_ARGS="${CV_ARGS} --total_folds ${TOTAL_FOLDS}"
              fi

              # Handle caching options for memory optimization
              CACHE_ARGS=""
              if [ "${METADATA_CACHE}" = "true" ]; then
                echo "Using metadata-only caching to reduce memory usage"
                CACHE_ARGS="--metadata_cache"
              else
                echo "Dataset caching disabled to reduce memory usage"
                CACHE_ARGS="--disable_cache"
              fi

              # GPU acceleration options
              GPU_ARGS=""
              if [ "${GPU_MEMORY_FRACTION}" != "" ]; then
                GPU_ARGS="--gpu_memory_fraction ${GPU_MEMORY_FRACTION}"
              fi

              if [ "${BATCH_GPU_CACHE}" = "true" ]; then
                GPU_ARGS="${GPU_ARGS} --batch_gpu_cache"
              fi

              if [ "${PREFETCH_FACTOR}" != "" ]; then
                GPU_ARGS="${GPU_ARGS} --prefetch_factor ${PREFETCH_FACTOR}"
              fi

              # Dataset initialization control arguments
              DATASET_INIT_ARGS=""
              if [ "${LAZY_INIT}" = "true" ]; then
                echo "Using lazy initialization for dataset"
                DATASET_INIT_ARGS="--lazy_init"
              else
                echo "Disabling lazy initialization for dataset (will pre-load all metadata)"
              fi
              
              # Small dataset argument
              SMALL_DATASET_ARG=""
              if [ "${SMALL_DATASET}" != "" ]; then
                echo "Using small dataset fraction: ${SMALL_DATASET}"
                SMALL_DATASET_ARG="--small_dataset ${SMALL_DATASET}"
              fi

              # Set up save directory
              SAVE_DIR="/mnt/storage/checkpoints/cv_kd"
              if [ "${SUB_DIR}" != "" ]; then
                SAVE_DIR="${SAVE_DIR}/${SUB_DIR}"
              fi
              echo "Using save directory: ${SAVE_DIR}"
              mkdir -p "${SAVE_DIR}"

              # Run the cross-validation with knowledge distillation script
              echo "Starting cross-validation with knowledge distillation training..."
              python train_cv_kd.py --config ./config/student_config.yaml \
                --save_dir ${SAVE_DIR} \
                --use_voca \
                ${MODEL_TYPE_ARGS} \
                ${MODEL_SCALE_ARG} \
                ${LR_ARGS} \
                ${LR_SCHEDULE_ARG} \
                ${WARMUP_ARG} \
                ${FOCAL_LOSS_ARG} \
                ${KD_LOSS_ARG} \
                ${CV_ARGS} \
                ${GPU_ARGS} \
                ${CACHE_ARGS} \
                ${DATASET_INIT_ARGS} \
                ${SMALL_DATASET_ARG}
          resources:
            limits:
              nvidia.com/gpu: "1"
              memory: 32Gi
              cpu: "8"
            requests:
              nvidia.com/gpu: "1"
              memory: 16Gi
              cpu: "4"
          volumeMounts:
            - name: storage
              mountPath: /mnt/storage
      volumes:
        - name: storage
          persistentVolumeClaim:
            claimName: chord-data-pvc
