# Student model configuration
# These are default values that can be overridden with --arg values or 
# environment variables (USE_FOCAL_LOSS, etc.) in the Kubernetes job spec.

# Training hyperparameters
training:
  batch_size: 128
  max_epochs: 100
  learning_rate: 1.0e-3  # Base learning rate
  min_learning_rate: 5.0e-6  # Minimum learning rate for schedulers
  weight_decay: 1.0e-5
  betas: [0.9, 0.98]
  epsilon: 1.0e-9
  early_stopping_patience: 9
  seq_len: 108
  seq_stride: 36
  save_checkpoint_interval: 10
  lr_decay_factor: 0.95
  # Warm-up parameters with expanded options
  use_warmup: true
  warmup_epochs: 10
  warmup_start_lr: 1.0e-5  # Initial learning rate at the start of warm-up
  warmup_end_lr: 1.0e-3    # Target learning rate at the end of warm-up (typically same as learning_rate)
  # LR schedule parameter (can be overridden by LR_SCHEDULE env var)
  lr_schedule: 'cosine'  # Can be 'cosine', 'linear_decay', 'one_cycle', or 'cosine_warm_restarts'
  # Focal loss parameters (can be overridden by USE_FOCAL_LOSS env var)
  use_focal_loss: true
  focal_gamma: 2.0
  focal_alpha: 0.25
  # Knowledge distillation parameters (can be overridden by USE_KD_LOSS env var)
  use_kd_loss: true
  kd_alpha: 0.9
  temperature: 3.0

# Model hyperparameters
model:
  scale: 1.0          # Scaling factor: 0.5 = half size, 1.0 = base size, 2.0 = double size
  base_config:        # Base configuration for scale=1.0
    f_layer: 3        # Base frequency attention layers 
    f_head: 6         # Base frequency attention heads
    t_layer: 3        # Base temporal attention layers
    t_head: 6         # Base temporal attention heads
    d_layer: 3        # Base decoder layers
    d_head: 6         # Base decoder heads
  n_group: 12         # Changed from 4 to 12 to ensure compatibility with n_bins=144 (must divide evenly)
  dropout: 0.3        # Dropout rate
  use_chord_aware_loss: false

mp3:
  song_hz: 22050
  inst_len: 10.0      # Updated to match teacher
  skip_interval: 5.0  # Added from teacher

feature:
  n_bins: 144         # Updated to match teacher
  bins_per_octave: 24 # Updated to match teacher
  hop_length: 2048    # Updated to match teacher
  hop_duration: 0.0928  # Computed as 2048 / 22050 â‰ˆ 0.0928 seconds per frame
  large_voca: true    # Changed to true to use large vocabulary by default

# Data paths
paths:
  # Storage root path - can be changed to point to different data storage locations
  storage_root: "/mnt/storage"
  
  # Data paths - can be absolute or relative to storage_root
  # If a path starts with "/", it's treated as absolute, otherwise relative to storage_root
  spec_dir: "data/synth/spectrograms"    # Will be resolved as {storage_root}/data/synth/spectrograms
  label_dir: "data/synth/labels"         # Will be resolved as {storage_root}/data/synth/labels
  
  # Alternative paths (used as fallbacks if primary paths don't have data)
  # Set to empty string or remove to disable
  alt_spec_dir: ""
  alt_label_dir: ""
  
  # Output paths
  checkpoints_dir: "checkpoints"

# Data parameters
# set both lazy_init to false and small_dataset_percentage to a percentage to use fraction of dataset
data:
  lazy_init: true           # Controls dataset lazy initialization
  cache_fraction: 0.1        # Fraction of dataset to cache (default: 0.1 = 10%)
  metadata_only: true        # Only cache metadata, not full spectrograms
  disable_cache: false       # Set to true to disable dataset caching
  # New parameter for small dataset testing
  small_dataset_percentage: null  # Set to a value like 0.01 for 1% or null to use full dataset

# Miscellaneous
misc:
  seed: 42
  use_cuda: true
  logging_level: 1
  augmentation_enabled: false